{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ConorD28/Baseball/blob/main/MLB_Playoffs_2024.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rq6xsPJUvkXO"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import math\n",
        "from sklearn.decomposition import PCA\n",
        "%matplotlib inline\n",
        "df_NP = pd.read_csv('inputs.csv')\n",
        "df = pd.read_csv('playoff_stats.csv')\n",
        "\n",
        "df_NP.isnull().sum().sum() #Check if there are NA values\n",
        "df.isnull().sum().sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Correlation/Scores**"
      ],
      "metadata": {
        "id": "l9zr28DwjQ6D"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2LtJLaEVvqfY"
      },
      "outputs": [],
      "source": [
        "import scipy.stats\n",
        "def correlation(dataset, threshold, target): #Function to get Pearson's correlation between input and target\n",
        "  data = []\n",
        "  i = 0\n",
        "  for col in dataset.columns:\n",
        "      cor2 = dataset.iloc[:,i].corr(target) #scipy.stats.spearmanr(x, y)[0] and scipy.stats.kendalltau(x, y)[0]\n",
        "      column_headers = list(dataset.columns.values)\n",
        "      if(abs(cor2) > threshold):\n",
        "        data.append(dataset.iloc[:,i]) #make list of columns that meet the threshold\n",
        "        print(col)\n",
        "        print(round(cor2,3))\n",
        "      i = i + 1\n",
        "  return data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ngjdaqWIvssV"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from numpy.random.mtrand import random_sample\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression, Ridge, RidgeCV, Lasso, LassoCV, ElasticNet, ElasticNetCV, MultiTaskLassoCV\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C5q3JY8L1e1M"
      },
      "outputs": [],
      "source": [
        "def Scores(y, y_pred, y_full):\n",
        "  MSE = mean_squared_error(y, y_pred)\n",
        "  MAE = mean_absolute_error(y, y_pred)\n",
        "  Normalized_RMSE = (np.sqrt(MSE)/np.mean(y_full))*100\n",
        "  Normalized_MAE = (MAE/np.mean(y_full))*100\n",
        "  Avg_Normalized_Score = (Normalized_RMSE + Normalized_MAE)/2\n",
        "  avg_error = (sum(abs(y_pred-y)))/len(y_pred)\n",
        "  print(f'Avg. Normalized Score:{ Avg_Normalized_Score:.1f}%')\n",
        "  #print(f'Normalized RMSE:{ Normalized_RMSE:.1f}%')\n",
        "  #print(f'Normalized MAE:{ Normalized_MAE:.2f}%')\n",
        "  #print(f'MAE:{ MAE:.3f}')\n",
        "  #print(f'RMSE:{ np.sqrt(MSE):.3f}')\n",
        "  print(f'Avg. Error:{avg_error:.4f}')\n",
        "  #print(y_pred-y)\n",
        "  return Avg_Normalized_Score"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **ML Algorithms**"
      ],
      "metadata": {
        "id": "WhvX9hZQi5U7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zLVl7ooxvt7q"
      },
      "outputs": [],
      "source": [
        "def RLE_Model(X, y, choice, predict_df): #Function to run Ridge, Lasso, or ElasticNet model\n",
        "  X_train, X_test, y_train, y_test = X[10:30], X[0:10], y[10:30], y[0:10]\n",
        "\n",
        "  if(choice==\"Ridge\"):\n",
        "    alphas = np.geomspace(1e-10, 1e10, num=100)\n",
        "    pipeline = make_pipeline(RidgeCV(alphas=alphas))\n",
        "    pipeline.fit(X_train, y_train)\n",
        "\n",
        "  if(choice==\"Lasso\"):\n",
        "    alphas = np.geomspace(1e-10, 1e10, num=100)\n",
        "    pipeline = make_pipeline(LassoCV(alphas=alphas))\n",
        "    pipeline.fit(X_train, y_train)\n",
        "\n",
        "  if(choice==\"Elastic\"):\n",
        "    l1_ratio = [0, 0.3, 0.5, 0.7, 0.9, 1]\n",
        "    alphas = np.geomspace(1e-10, 1e10, num=100)\n",
        "    pipeline = make_pipeline(ElasticNetCV(alphas=alphas, l1_ratio=l1_ratio, max_iter=100000))\n",
        "    pipeline.fit(X_train, y_train)\n",
        "\n",
        "  #print(f'Chosen alpha  {pipeline.steps[0][1].alpha_:.6f}')\n",
        "  #print(f'Intercept (b) {pipeline.steps[0][1].intercept_:.6f}')\n",
        "  #print(pd.Series(pipeline.steps[0][1].coef_, index=X.columns),'\\n')\n",
        "\n",
        "  #Calculate the predicted values:\n",
        "  y_train_pred = pipeline.predict(X_train)\n",
        "  #print(y_train_pred)\n",
        "  print()\n",
        "\n",
        "  y_test_pred = pipeline.predict(X_test)\n",
        "\n",
        "  #Training Scores:\n",
        "  Avg_N_Score_train = Scores(y_train, y_train_pred, y)\n",
        "  #print()\n",
        "\n",
        "  #Test Predictions:\n",
        "  print(\"Test predictions:\")\n",
        "  print(y_test_pred)\n",
        "  #print()\n",
        "\n",
        "  #Testing Scores:\n",
        "  Avg_N_Score_test = Scores(y_test, y_test_pred, y)\n",
        "  print(f'Difference of avg scores:{ Avg_N_Score_test - Avg_N_Score_train:.2f}%') #Difference between testing and traing scores to check if my bias-variance tradeoff is good\n",
        "  print()\n",
        "\n",
        "  #print(sum(abs(y_test_pred - y_test))/10)\n",
        "\n",
        "  #Predict:\n",
        "  predictions = pipeline.predict(predict_df)\n",
        "\n",
        "  return y_test_pred, y_train_pred, predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FpU87Mgvvx0s"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import make_regression\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "def GBR_model(X,y, t, l, n, predict_df):\n",
        "  X_train, X_test, y_train, y_test = X[10:30], X[0:10], y[10:30], y[0:10]\n",
        "  reg = GradientBoostingRegressor(tol = t, learning_rate = l, n_estimators=n, random_state=0) #default: tol = 0.0001, learning rate - 0.1, 100, friedman_mse\n",
        "  reg.fit(X_train, y_train)\n",
        "  y_train_pred = reg.predict(X_train)\n",
        "  #print(y_train_pred)\n",
        "\n",
        "  y_test_pred = reg.predict(X_test)\n",
        "\n",
        "  #Training Scores:\n",
        "  Avg_N_Score_train = Scores(y_train, y_train_pred, y)\n",
        "  #print()\n",
        "\n",
        "  #Predictions:\n",
        "  print(\"Test predictions:\")\n",
        "  print(y_test_pred)\n",
        "  #print()\n",
        "\n",
        "  #Testing Scores:\n",
        "  Avg_N_Score_test = Scores(y_test, y_test_pred, y)\n",
        "  print(f'Difference of avg scores:{ Avg_N_Score_test - Avg_N_Score_train:.2f}%') #Difference between testing and traing scores to check if my bias-variance tradeoff is good\n",
        "  print()\n",
        "\n",
        "  #Predict:\n",
        "  predictions = reg.predict(predict_df)\n",
        "  return y_test_pred, y_train_pred, predictions"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import BaggingRegressor\n",
        "import xgboost as xgb\n",
        "import pickle\n",
        "\n",
        "def BR_model(X,y, choice):\n",
        "  X_train, X_test, y_train, y_test = X[10:30], X[0:10], y[10:30], y[0:10]\n",
        "  if choice == \"Boost\":\n",
        "    reg = BaggingRegressor(estimator=xgb.XGBRegressor()) #\n",
        "  else:\n",
        "    reg = BaggingRegressor()\n",
        "  #reg = pickle.load(open(\"BR_model Per 100\", \"rb\"))\n",
        "  reg.fit(X_train, y_train)\n",
        "  y_train_pred = reg.predict(X_train)\n",
        "  #print(y_train_pred)\n",
        "\n",
        "  y_test_pred = reg.predict(X_test)\n",
        "\n",
        "  #Training Scores:\n",
        "  Avg_N_Score_train = Scores(y_train, y_train_pred, y)\n",
        "  #print()\n",
        "\n",
        "  #Predictions:\n",
        "  print(\"Test predictions:\")\n",
        "  print(y_test_pred)\n",
        "  #print()\n",
        "\n",
        "  #Testing Scores:\n",
        "  Avg_N_Score_test = Scores(y_test, y_test_pred, y)\n",
        "  print(f'Difference of avg scores:{ Avg_N_Score_test - Avg_N_Score_train:.2f}%') #Difference between testing and traing scores to check if my bias-variance tradeoff is good\n",
        "  print()\n",
        "\n",
        "  #Predict:\n",
        "  #predictions = reg.predict(predict_df)\n",
        "  #print(predictions)\n",
        "\n",
        "  pickle.dump(reg, open(\"BR_model RA_Batter2\", \"wb\"))\n",
        "  return y_test_pred, y_train_pred"
      ],
      "metadata": {
        "id": "ppib1skHfgGk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestRegressor\n",
        "\n",
        "def RF_model(X,y, criterion, trees):\n",
        "  X_train, X_test, y_train, y_test = X[10:30], X[0:10], y[10:30], y[0:10]\n",
        "  if criterion == \"aboslute\":\n",
        "    reg = RandomForestRegressor(n_estimators = trees, criterion = \"absolute_error\") #\n",
        "  elif criterion == \"friedman\":\n",
        "    reg = RandomForestRegressor(n_estimators = trees, criterion = \"friedman_mse\") #\n",
        "  elif criterion == \"poisson\":\n",
        "    reg = RandomForestRegressor(n_estimators = trees, criterion = \"poisson\") #\n",
        "  else:\n",
        "    reg = RandomForestRegressor(n_estimators = trees)\n",
        "  #reg = pickle.load(open(\"BR_model Per 100\", \"rb\"))\n",
        "  reg.fit(X_train, y_train)\n",
        "  y_train_pred = reg.predict(X_train)\n",
        "  #print(y_train_pred)\n",
        "\n",
        "  y_test_pred = reg.predict(X_test)\n",
        "\n",
        "  #Training Scores:\n",
        "  Avg_N_Score_train = Scores(y_train, y_train_pred, y)\n",
        "  #print()\n",
        "\n",
        "  #Predictions:\n",
        "  print(\"Test predictions:\")\n",
        "  print(y_test_pred)\n",
        "  #print()\n",
        "\n",
        "  #Testing Scores:\n",
        "  Avg_N_Score_test = Scores(y_test, y_test_pred, y)\n",
        "  print(f'Difference of avg scores:{ Avg_N_Score_test - Avg_N_Score_train:.2f}%') #Difference between testing and traing scores to check if my bias-variance tradeoff is good\n",
        "  print()\n",
        "\n",
        "  #Predict:\n",
        "  #predictions = reg.predict(predict_df)\n",
        "  #print(predictions)\n",
        "\n",
        "  pickle.dump(reg, open(\"RF_model RA_Batter\", \"wb\"))\n",
        "  return y_test_pred, y_train_pred"
      ],
      "metadata": {
        "id": "NfB2M7eKfne7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XfM5w0Ncvynu"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import SGDRegressor\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "#SGD Regressor:\n",
        "def SGD_model(X,y, t, ep):\n",
        "\n",
        "  reg = make_pipeline(SGDRegressor(max_iter=1000, tol=t, epsilon = ep)) #tol = 0.001, epsilon=0.1\n",
        "  X_train, X_test, y_train, y_test = X[10:30], X[0:10], y[10:30], y[0:10]\n",
        "  reg.fit(X_train, y_train)\n",
        "  y_train_pred = reg.predict(X_train)\n",
        "  #print(y_train_pred)\n",
        "\n",
        "  y_test_pred = reg.predict(X_test)\n",
        "\n",
        "  #Training Scores:\n",
        "  Avg_N_Score_train = Scores(y_train, y_train_pred, y)\n",
        "  #print()\n",
        "\n",
        "  #Predictions:\n",
        "  print(\"Test predictions:\")\n",
        "  print(y_test_pred)\n",
        "  #print()\n",
        "\n",
        "  #Testing Scores:\n",
        "  Avg_N_Score_test = Scores(y_test, y_test_pred, y)\n",
        "  print(f'Difference of avg scores:{ Avg_N_Score_test - Avg_N_Score_train:.2f}%') #Difference between testing and traing scores to check if my bias-variance tradeoff is good\n",
        "  print()\n",
        "\n",
        "  #predictions = reg.predict(predict_df)\n",
        "  #print(predictions)\n",
        "  pickle.dump(reg, open(\"SGD_model RA_Batter\", \"wb\"))\n",
        "  return y_test_pred, y_train_pred"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Keras Sequential Neural Net\n",
        "import tensorflow\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Activation\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "early_stop = EarlyStopping(monitor='loss', mode='min', verbose=1, patience=2)\n",
        "\n",
        "def Keras_model(X,y,e, u, u2, u3, u4, u5, choice):\n",
        "  X_train, X_test, y_train, y_test = X[10:30], X[0:10], y[10:30], y[0:10]\n",
        "  model = Sequential()\n",
        "  model.add(Dense(units=u, input_dim=X_train.shape[1], activation='relu')) # Hidden 1, 60\n",
        "  model.add(Dense(units=u2,activation='relu')) # Hidden 2, 30\n",
        "  model.add(Dense(units=u3,activation='relu'))\n",
        "  model.add(Dense(units=u4,activation='relu'))\n",
        "  model.add(Dense(units=u5,activation='relu'))\n",
        "  model.add(Dense(units=15,activation='relu')) #15\n",
        "  model.add(Dense(units=1)) #,activation='relu'\n",
        "  model.compile(loss='mean_squared_error', optimizer=choice) #\n",
        "  model.fit(X_train, y_train, verbose=0, epochs=e, callbacks=[early_stop]); #callbacks=[early_stop]\n",
        "\n",
        "  y_train_pred = model.predict(X_train)\n",
        "  y_train_pred = y_train_pred.flatten()\n",
        "\n",
        "  y_test_pred = model.predict(X_test)\n",
        "  y_test_pred = y_test_pred.flatten()\n",
        "\n",
        "  #Training Scores:\n",
        "  Avg_N_Score_train = Scores(y_train, y_train_pred, y)\n",
        "  #print()\n",
        "\n",
        "  #Predictions:\n",
        "  print(\"Test predictions:\")\n",
        "  print(y_test_pred)\n",
        "  #print()\n",
        "\n",
        "  #Testing Scores:\n",
        "  Avg_N_Score_test = Scores(y_test, y_test_pred, y)\n",
        "  print(f'Difference of avg scores:{ Avg_N_Score_test - Avg_N_Score_train:.2f}%') #Difference between testing and traing scores to check if my bias-variance tradeoff is good\n",
        "  print()\n",
        "\n",
        "  model.save('/content/drive/MyDrive/Models/Keras_Model.h5')"
      ],
      "metadata": {
        "id": "Zzt-o4jPgtdH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.tree import DecisionTreeRegressor\n",
        "\n",
        "def DTR_model(X,y,leafs):\n",
        "  X_train, X_test, y_train, y_test = X[10:30], X[0:10], y[10:30], y[0:10]\n",
        "  # We introduce regularization by increasing the value of min_samples_leaf\n",
        "  tree_reg_regularized = DecisionTreeRegressor(random_state=42, min_samples_leaf=leafs)\n",
        "  tree_reg_regularized.fit(X_train, y_train)\n",
        "  y_train_pred = tree_reg_regularized.predict(X_train) #_regularized\n",
        "  #print(y_train_pred)\n",
        "\n",
        "  y_test_pred = tree_reg_regularized.predict(X_test) #_regularized\n",
        "\n",
        "  #Training Scores:\n",
        "  Avg_N_Score_train = Scores(y_train, y_train_pred, y)\n",
        "  #print()\n",
        "\n",
        "  #Predictions:\n",
        "  print(\"Test predictions:\")\n",
        "  print(y_test_pred)\n",
        "  #print()\n",
        "\n",
        "  #Testing Scores:\n",
        "  Avg_N_Score_test = Scores(y_test, y_test_pred, y)\n",
        "  print(f'Difference of avg scores:{ Avg_N_Score_test - Avg_N_Score_train:.2f}%') #Difference between testing and traing scores to check if my bias-variance tradeoff is good\n",
        "  print()"
      ],
      "metadata": {
        "id": "sOg9t7czmLFe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.svm import LinearSVR\n",
        "\n",
        "def SVM_model(X,y,ep, predict_df):\n",
        "  X_train, X_test, y_train, y_test = X[10:30], X[0:10], y[10:30], y[0:10]\n",
        "  svm_reg = LinearSVR(epsilon=ep, random_state=42) #default: epsilon = 0 tol=0.0001, C=1.0\n",
        "  svm_reg.fit(X_train, y_train)\n",
        "\n",
        "  #Train Predictions:\n",
        "  y_train_pred = svm_reg.predict(X_train)\n",
        "  #print(y_train_pred)\n",
        "\n",
        "  #Training Scores:\n",
        "  Avg_N_Score_train = Scores(y_train, y_train_pred, y)\n",
        "  #print()\n",
        "\n",
        "  #Test Predictions:\n",
        "  y_test_pred = svm_reg.predict(X_test)\n",
        "  print(\"Test predictions:\")\n",
        "  print(y_test_pred)\n",
        "  #print()\n",
        "\n",
        "  #Testing Scores:\n",
        "  Avg_N_Score_test = Scores(y_test, y_test_pred, y)\n",
        "  print(f'Difference of avg scores:{ Avg_N_Score_test - Avg_N_Score_train:.2f}%') #Difference between testing and traing scores to check if my bias-variance tradeoff is good\n",
        "  print()\n",
        "\n",
        "  final_preds = svm_reg.predict(predict_df)\n",
        "\n",
        "  return y_test_pred, y_train_pred, final_preds"
      ],
      "metadata": {
        "id": "luksQpOa9mx6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.svm import SVR\n",
        "\n",
        "def SVM_models(X,y, choice, ep, C_value, predict_df):\n",
        "  X_train, X_test, y_train, y_test = X[10:30], X[0:10], y[10:30], y[0:10]\n",
        "\n",
        "  if(choice==\"rbf\"):\n",
        "    model = SVR(kernel=\"rbf\", C=C_value, gamma=0.1, epsilon=ep) #0.1 default ep; 100 default C, 0.1 default gamma\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "  if(choice==\"poly\"):\n",
        "    model = SVR(kernel=\"poly\", C=C_value, gamma=\"auto\", degree=3, epsilon=ep, coef0=1) #0.1 default ep; 100 default C\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "  if(choice == \"linear\"):\n",
        "    model = SVR(kernel=\"linear\", C=C_value, gamma=\"auto\", degree=3, epsilon=ep, coef0=1) #0.1 default ep; 100 default C\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "  #Train Predictions:\n",
        "  y_train_pred = model.predict(X_train)\n",
        "  #print(y_train_pred)\n",
        "\n",
        "  #Training Scores:\n",
        "  Avg_N_Score_train = Scores(y_train, y_train_pred, y)\n",
        "  #print()\n",
        "\n",
        "  #Test Predictions:\n",
        "  y_test_pred = model.predict(X_test)\n",
        "  print(\"Test predictions:\")\n",
        "  print(y_test_pred)\n",
        "  #print()\n",
        "\n",
        "  #Testing Scores:\n",
        "  Avg_N_Score_test = Scores(y_test, y_test_pred, y)\n",
        "  print(f'Difference of avg scores:{ Avg_N_Score_test - Avg_N_Score_train:.2f}%') #Difference between testing and traing scores to check if my bias-variance tradeoff is good\n",
        "  print()\n",
        "\n",
        "  final_preds = model.predict(predict_df)\n",
        "\n",
        "  return y_test_pred, y_train_pred, final_preds"
      ],
      "metadata": {
        "id": "1ms2SSQ_3ncH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Inputs**"
      ],
      "metadata": {
        "id": "k4QbyjhZi-0Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import make_swiss_roll\n",
        "from sklearn.manifold import LocallyLinearEmbedding"
      ],
      "metadata": {
        "id": "UhxkZpeNTZA7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_inputs(data_frame, full_data_frame, y_col, scaler_choice, thresh):\n",
        "#Feature Importance:\n",
        "  if scaler_choice == \"MinMax\":\n",
        "    scaler = MinMaxScaler()\n",
        "  else:\n",
        "    scaler = StandardScaler()\n",
        "\n",
        "  data_scaled = pd.DataFrame(scaler.fit_transform(data_frame[10:30]), columns = data_frame.columns)\n",
        "  df_20 = full_data_frame[10:30] #for training target\n",
        "  data_correlated = correlation(data_scaled, thresh, df_20[y_col]) #.115 lowest for All csv\n",
        "  data_correlated_df = pd.DataFrame(data_correlated)\n",
        "  data_correlated_df2 = data_correlated_df.transpose() #Correlated inputs\n",
        "\n",
        "#Train test split and scale:\n",
        "  X = data_frame.loc[:, data_correlated_df2.columns] #get non scaled data with important features\n",
        "  X_train, X_test = X[10:30], X[0:10]\n",
        "  X_train_processed = pd.DataFrame(scaler.fit_transform(X_train), columns = X_train.columns)\n",
        "  X_test_processed = pd.DataFrame(scaler.transform(X_test), columns = X_test.columns)\n",
        "  correlated_scaled_data = pd.merge(X_test_processed, X_train_processed, how = 'outer')\n",
        "\n",
        "  print(len(X.columns))\n",
        "  return correlated_scaled_data, scaler"
      ],
      "metadata": {
        "id": "Z0RXs3FQMoHz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def reduce_df(data, reduction_choice):\n",
        "  X_train, X_test = data[10:30], data[0:10]\n",
        "\n",
        "  pca=PCA(n_components = 2, random_state=0)\n",
        "  lle = LocallyLinearEmbedding(n_components=2, n_neighbors=5, random_state=42)\n",
        "\n",
        "  if reduction_choice == \"PCA\":\n",
        "    #pca=PCA(n_components = 2, random_state=0) #n_components = None, 2\n",
        "    X_train_PCA = pca.fit_transform(X_train)\n",
        "    X_train_PCA_df = pd.DataFrame(X_train_PCA)\n",
        "    X_test_PCA = pca.transform(X_test)\n",
        "    X_test_PCA_df = pd.DataFrame(X_test_PCA)\n",
        "    data_PCA = pd.merge(X_test_PCA_df, X_train_PCA_df, how = 'outer')\n",
        "    X = data_PCA\n",
        "    print(\"Principal axes:\\n\", pca.components_.tolist())\n",
        "    print(\"Explained variance:\\n\", pca.explained_variance_.tolist())\n",
        "    print(\"Mean:\", pca.mean_)\n",
        "\n",
        "  else:\n",
        "    #lle = LocallyLinearEmbedding(n_components=2, n_neighbors=5, random_state=42) #n_components=2 is default, neighbors 5 is default\n",
        "    X_unrolled_train = lle.fit_transform(X_train)\n",
        "    X_train_LLE_df = pd.DataFrame(X_unrolled_train)\n",
        "    X_unrolled_test = lle.transform(X_test)\n",
        "    X_test_LLE_df = pd.DataFrame(X_unrolled_test)\n",
        "    data_LLE = pd.merge(X_test_LLE_df, X_train_LLE_df, how = 'outer')\n",
        "    X = data_LLE\n",
        "\n",
        "  return X, pca, lle"
      ],
      "metadata": {
        "id": "dZJCNs4vc_FN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=RuntimeWarning)"
      ],
      "metadata": {
        "id": "h5svcGPYkLsK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_NP = df_NP.apply(pd.to_numeric, errors='coerce')"
      ],
      "metadata": {
        "id": "NmMgJiXwklCb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inputs_MMS, scaler_RA_Batter_MMS = get_inputs(df_NP, df, \"RA/BatterPitching/More In Playoffs\", \"MinMax\", .855) #MinMax,"
      ],
      "metadata": {
        "id": "hqYXYqijmqsX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inputs, scaler_RA_Batter = get_inputs(df_NP, df, \"RA/BatterPitching/More In Playoffs\", \"\", .855)"
      ],
      "metadata": {
        "id": "1AT0nAvGnSf8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inputs_MMS_PCA = reduce_df(inputs_MMS, \"PCA\")[0]"
      ],
      "metadata": {
        "collapsed": true,
        "id": "eSh6-bvaQhFp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inputs_PCA = reduce_df(inputs, \"PCA\")[0]"
      ],
      "metadata": {
        "id": "Tlvr_HJOek9-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inputs_LLE = reduce_df(inputs, \"\")[0]"
      ],
      "metadata": {
        "id": "QbOYrdGFe5P6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inputs_MMS_LLE = reduce_df(inputs_MMS, \"\")[0]"
      ],
      "metadata": {
        "id": "S57MIqhAKZAk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Y = df[\"RA/BatterPitching/More In Playoffs\"] #RA/GMPitching/More In Playoffs"
      ],
      "metadata": {
        "id": "Lht-olFUlqsC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Models**"
      ],
      "metadata": {
        "id": "N3Vyz8ZFjD1g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "RLE_Model(inputs, Y, \"Ridge\", inputs)\n",
        "RLE_Model(inputs, Y, \"Lasso\", inputs)\n",
        "SGD_model(inputs, Y, 1e-3, 0.1) #\n",
        "print(\"GBR and DTR:\")\n",
        "GBR_model(inputs, Y, .0001, 0.01, 100, inputs) #\n",
        "DTR_model(inputs, Y, 100) #\n",
        "print(\"Random Forest:\")\n",
        "RF_model(inputs, Y, \"absolute\", 100) #\n",
        "RF_model(inputs, Y, \"friedman\", 100) #\n",
        "RF_model(inputs, Y, \"poisson\", 100) #\n",
        "RF_model(inputs, Y, \"\", 100) #"
      ],
      "metadata": {
        "id": "sZiNfyW6tIpt",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "SVM_models(inputs, Y, \"rbf\", .1, 100, inputs) #\n",
        "SVM_models(inputs, Y, \"poly\", .1, 100, inputs) #\n",
        "SVM_models(inputs, Y, \"linear\", .1, 100, inputs)\n",
        "SVM_model(inputs,Y, 0.0, inputs) #"
      ],
      "metadata": {
        "id": "9eGYjBc_-JxT",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "RLE_Model(inputs, Y, \"Elastic\", inputs) #"
      ],
      "metadata": {
        "id": "9sZlCeWYWM01",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "BR_model(inputs,Y, \"Boost\") #\n",
        "BR_model(inputs,Y, \"\") #\n",
        "\n",
        "inputs2 = inputs.astype(np.float32)\n",
        "Y2 = Y.astype(np.float32)\n",
        "Keras_model(inputs2, Y2, 200, 120, 60, 30, 20, 15, \"adamax\") #\n",
        "Keras_model(inputs2, Y2, 200, 120, 60, 30, 20, 15, \"adam\") #\n",
        "Keras_model(inputs2, Y2, 200, 120, 60, 30, 20, 15, \"nadam\") #"
      ],
      "metadata": {
        "id": "pfrc4zA3hnYP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**PCA:**"
      ],
      "metadata": {
        "id": "AqB-Izr3ojui"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "RLE_Model(inputs_PCA, Y, \"Ridge\", inputs_PCA) #\n",
        "RLE_Model(inputs_PCA, Y, \"Lasso\", inputs_PCA) #\n",
        "SGD_model(inputs_PCA, Y, 1e-3, 0.1) #\n",
        "print(\"GBR and DTR:\")\n",
        "GBR_model(inputs_PCA, Y, .0001, 0.01, 100, inputs_PCA) #\n",
        "DTR_model(inputs_PCA, Y, 100) #\n",
        "print(\"Random Forest:\")\n",
        "RF_model(inputs_PCA, Y, \"absolute\", 100) #\n",
        "RF_model(inputs_PCA, Y, \"friedman\", 100) #\n",
        "RF_model(inputs_PCA, Y, \"poisson\", 100) #\n",
        "RF_model(inputs_PCA, Y, \"\", 100) #"
      ],
      "metadata": {
        "id": "RHgvSBMnoZfI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "SVM_models(inputs_PCA, Y, \"rbf\", 0.01, 100, inputs_PCA)"
      ],
      "metadata": {
        "id": "mwVi7rKlTMBZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "SVM_models(inputs_PCA, Y, \"rbf\", .1, 100, inputs_PCA) #\n",
        "SVM_models(inputs_PCA, Y, \"poly\", .1, 100, inputs_PCA) #\n",
        "SVM_models(inputs_PCA, Y, \"linear\", .1, 100, inputs_PCA) #\n",
        "SVM_model(inputs_PCA,Y, 0.0, inputs_PCA) #"
      ],
      "metadata": {
        "id": "TFfp-q4yQGYg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "RLE_Model(inputs_PCA, Y, \"Elastic\", inputs_PCA) #"
      ],
      "metadata": {
        "id": "FTq-Rfopopqu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "BR_model(inputs_PCA,Y, \"Boost\")\n",
        "BR_model(inputs_PCA,Y, \"\")\n",
        "\n",
        "inputs2 = inputs_PCA.astype(np.float32)\n",
        "Y2 = Y.astype(np.float32)\n",
        "Keras_model(inputs2, Y2, 200, 120, 60, 30, 20, 15, \"adamax\") #\n",
        "Keras_model(inputs2, Y2, 200, 120, 60, 30, 20, 15, \"adam\") #\n",
        "Keras_model(inputs2, Y2, 200, 120, 60, 30, 20, 15, \"nadam\") #"
      ],
      "metadata": {
        "id": "30tUhw1poqyz",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**LLE:**"
      ],
      "metadata": {
        "id": "i31bRl4ho89G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "RLE_Model(inputs_LLE, Y, \"Ridge\", inputs_LLE) #\n",
        "RLE_Model(inputs_LLE, Y, \"Lasso\", inputs_LLE) #\n",
        "SGD_model(inputs_LLE, Y, 1e-3, 0.1) #\n",
        "print(\"GBR and DTR:\")\n",
        "GBR_model(inputs_LLE, Y, .0001, 0.01, 100, inputs_LLE) #\n",
        "DTR_model(inputs_LLE, Y, 100) #\n",
        "print(\"Random Forest:\")\n",
        "RF_model(inputs_LLE, Y, \"absolute\", 100) #\n",
        "RF_model(inputs_LLE, Y, \"friedman\", 100) #\n",
        "RF_model(inputs_LLE, Y, \"poisson\", 100) #\n",
        "RF_model(inputs_LLE, Y, \"\", 100) #"
      ],
      "metadata": {
        "collapsed": true,
        "id": "uRvA3tlQo_ih"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "SVM_models(inputs_LLE, Y, \"rbf\", .01, 100, inputs_LLE) #\n",
        "SVM_models(inputs_LLE, Y, \"poly\", .04, 100, inputs_LLE) #\n",
        "SVM_models(inputs_LLE, Y, \"linear\", .01, 100, inputs_LLE) #\n",
        "SVM_model(inputs_LLE,Y, 0.01, inputs_LLE) #"
      ],
      "metadata": {
        "id": "cIYkoRulpB5n",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "RLE_Model(inputs_LLE, Y, \"Elastic\", inputs_LLE) #"
      ],
      "metadata": {
        "collapsed": true,
        "id": "vnw-Ol9zpDFE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "BR_model(inputs_LLE,Y, \"Boost\") #\n",
        "BR_model(inputs_LLE,Y, \"\") #\n",
        "\n",
        "inputs2 = inputs_LLE.astype(np.float32)\n",
        "Y2 = Y.astype(np.float32)\n",
        "Keras_model(inputs2, Y2, 200, 120, 60, 30, 20, 15, \"adamax\") #\n",
        "Keras_model(inputs2, Y2, 200, 120, 60, 30, 20, 15, \"adam\") #\n",
        "Keras_model(inputs2, Y2, 200, 120, 60, 30, 20, 15, \"nadam\") #"
      ],
      "metadata": {
        "collapsed": true,
        "id": "HtUgfoRGpEG2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Blenders/Predictions:**"
      ],
      "metadata": {
        "id": "VehwLHXWo7Nl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#RA/Batter Blender: #11.5, 4.8 - 13.9, .097 avg error, .855 to get 59 cols\n",
        "loaded_BR_model = pickle.load(open(\"BR_model RA_Batter\", \"rb\")) #\n",
        "loaded_BR_model2 = pickle.load(open(\"BR_model RA_Batter2\", \"rb\")) #\n",
        "\n",
        "RA_Batter = pd.read_csv('RA_Batter.csv')\n",
        "RA_Batter_scaled = scaler_RA_Batter.transform(RA_Batter)\n",
        "RA_Batter_scaled_MMS = scaler_RA_Batter_MMS.transform(RA_Batter)\n",
        "RA_Batter_PCA = (reduce_df(inputs, \"PCA\")[1]).transform(RA_Batter_scaled)\n",
        "RA_Batter_PCA_MMS = (reduce_df(inputs_MMS, \"PCA\")[1]).transform(RA_Batter_scaled_MMS)\n",
        "\n",
        "preds = (loaded_BR_model2.predict(inputs_MMS_PCA[0:10]), loaded_BR_model2.predict(inputs_MMS_PCA[10:30]),\n",
        "         loaded_BR_model2.predict(RA_Batter_PCA_MMS)) #11.4, 4.7, MMS - 13.8\n",
        "preds2 = SVM_models(inputs_PCA, Y, \"rbf\", 0.01, 100, RA_Batter_PCA) #12.3, 3.7 - 14.2\n",
        "preds3 = (loaded_BR_model.predict(inputs_MMS_PCA[0:10]), loaded_BR_model.predict(inputs_MMS_PCA[10:30]),\n",
        "          loaded_BR_model.predict(RA_Batter_PCA_MMS)) #11.4, 5.8, MMS - 14.3\n",
        "#preds4 = RLE_Model(inputs, Y, \"Ridge\", RA_Batter_scaled) #14.1, .3  -14.3\n",
        "#preds5 = RLE_Model(inputs_MMS_PCA, Y, \"Lasso\", RA_Batter_PCA_MMS)#14.5, .3, MMS - 14.7\n",
        "#preds6 = RLE_Model(inputs_PCA, Y, \"Lasso\", inputs_PCA) #14.6, .5 - 14.9\n",
        "#preds7 = RLE_Model(inputs_MMS, Y, \"Lasso\", inputs_MMS) #15, 2.3, MMS - 16.2\n",
        "\n",
        "train_preds = (preds[1] + preds2[1] + preds3[1])/3\n",
        "\n",
        "test_preds = (preds[0] + preds2[0] + preds3[0])/3\n",
        "\n",
        "RA_Batter_preds_2024 = (preds[2] + preds2[2] + preds3[2])/3\n",
        "\n",
        "print(\"Blender Train Scores then Test Scores:\")\n",
        "Scores(Y[10:30], train_preds, Y)\n",
        "print()\n",
        "Scores(Y[0:10], test_preds, Y)\n",
        "print()\n",
        "print(\"2024 RA/Batter Predictions:\")\n",
        "print(RA_Batter_preds_2024)\n",
        "RA_Batter_preds_2024_df = pd.DataFrame(RA_Batter_preds_2024)\n",
        "RA_Batter_preds_2024_df.to_excel(\"2024 RA_Batter Predictions.xlsx\")"
      ],
      "metadata": {
        "id": "R0u4eNaPa1Gw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "outputId": "296924d9-35ed-4e42-9a1e-b8f8efca96a8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Principal axes:\n",
            " [[0.14375503761905387, 0.08900768957737851, 0.14381144218535968, 0.13053624430582864, 0.08478513952922558, 0.14184531236925382, 0.10628508626927799, 0.1280643235682248, 0.08987502027116795, 0.1442246016261102, 0.13098085378781363, 0.08568499048682596, 0.14226020579654464, 0.15013431960709492, 0.1377849733866141, 0.14850430576183685, 0.1345110829602761, 0.1070008951007915, 0.1294140707621743, 0.12857734089733824, 0.13703444753445881, 0.13441092503411728, 0.10928606964996353, 0.10866254914064576, 0.14005091988237145, 0.11764024460166357, 0.1292233049820064, 0.13656376488872463, 0.15272309226679467, 0.12746467749121693, 0.11418440901862337, 0.13370134742178522, 0.15105083939882963, 0.12821947417266188, 0.14065957214196073, 0.12293492362081027, 0.13241903449632172, 0.12772698983938433, 0.1565853035400834, 0.12008925912160485, 0.13017706456626016, 0.1270503480544868, 0.15513916612329431, 0.12725355387047588, 0.13574738515604978, 0.13300739705258913, 0.10866254919795368, 0.1080139339352423, 0.13882800663271502, 0.11703935973651008, 0.12846055649870158, 0.14967134831169024, 0.13554078437950423, 0.15243974467396895, 0.12643764987275471, 0.11356621550648191, 0.13276232279056496, 0.15077246021360785, 0.12717699143693634], [0.047402209029814846, 0.23705913679975651, 0.04003953176612427, 0.06398346908452154, 0.24645722660858754, 0.04514669108686888, 0.21964392924954554, -0.14575327358290352, 0.2362241565224637, 0.03978527895555107, 0.06356878066334237, 0.24554974833892423, 0.044865947959834215, 0.03147175851257809, 0.053871683853314214, 0.03634691186371308, 0.05897393533550403, 0.21861232972730293, -0.14482715029292184, -0.1430670113310757, -0.06178280240643299, -0.1159862932612509, 0.09908307803664508, 0.09973604754043046, -0.05700878457996291, 0.19504072292997218, -0.15173032883265977, -0.10340022644467767, 0.017150347215400513, -0.1494394806360691, 0.20327696902912912, -0.0991253958170635, 0.021346920083276544, -0.1523163480384004, -0.12541428648596148, 0.1756762872205466, -0.10746093732185724, -0.10512839152174475, 0.009988543531468639, 0.18378586579884992, -0.10481458921795538, -0.10350977956190043, 0.01396160324828803, -0.14929286364542285, -0.06249470784402129, -0.11692382299557522, 0.09973604749859427, 0.10037469693493828, -0.05761832568457292, 0.19600751624214774, -0.1543414531195781, -0.11070413440577369, -0.10490195418797531, 0.01735247811407482, -0.15025348046268977, 0.20433387406705822, -0.10093773942943894, 0.021569293822517543, -0.15333008953140975]]\n",
            "Explained variance:\n",
            " [34.81976059626131, 10.579585378281843]\n",
            "Mean: [ 1.07691633e-15 -5.44009282e-16  2.66453526e-16 -1.21569421e-15\n",
            " -1.11022302e-16 -3.33066907e-17 -1.22124533e-16 -1.66533454e-16\n",
            "  5.10702591e-16 -6.43929354e-16  8.32667268e-17  1.13242749e-15\n",
            "  3.33066907e-16  9.21485110e-16 -4.94049246e-16 -3.10862447e-16\n",
            " -8.04911693e-16 -3.88578059e-16 -1.75415238e-15  0.00000000e+00\n",
            "  5.10702591e-16 -2.05946371e-15  1.44328993e-16  4.77395901e-16\n",
            "  1.42108547e-15  8.88178420e-16 -2.97123437e-15 -2.01783035e-15\n",
            "  4.32986980e-16 -1.83741911e-15 -1.48769885e-15 -1.20597976e-15\n",
            " -1.55431223e-16  2.20379270e-15  1.03805853e-15  5.10702591e-16\n",
            " -1.10467191e-15  2.44734788e-15  1.44328993e-16 -4.66293670e-16\n",
            "  7.88258347e-16  2.43416398e-15  1.13242749e-15 -9.99200722e-17\n",
            "  2.30926389e-15  1.62092562e-15 -1.07691633e-15  4.99600361e-16\n",
            " -2.45359288e-15 -3.88578059e-16  6.29635233e-15 -1.37667655e-15\n",
            "  1.09634524e-15 -3.33066907e-17  0.00000000e+00  1.44328993e-16\n",
            "  1.88460358e-15 -3.33066907e-17  2.33146835e-16]\n",
            "Principal axes:\n",
            " [[0.12092527712628394, 0.08404666001105984, 0.11794710078985496, 0.09864513379360974, 0.07922790590545907, 0.11784202813331617, 0.09709413332330437, 0.14895500596374073, 0.08519450594456085, 0.11803041136398629, 0.09945066940123404, 0.08041069191520378, 0.11792817682812556, 0.1253923740133614, 0.10890534744989402, 0.12556742607770682, 0.10395607236786689, 0.0982079048098347, 0.15153282843814023, 0.15454582289225488, 0.1547499249969951, 0.15124457401299551, 0.11355131869138234, 0.1123306807259016, 0.15365098476404232, 0.11092102249443143, 0.1472413889616604, 0.14610545998215194, 0.12568638842821403, 0.16872686029088002, 0.10703094065062017, 0.143253445101306, 0.1259100678412863, 0.15689353038594916, 0.15935892135800458, 0.11667149565240884, 0.14306769856388485, 0.12667597117088383, 0.13229592797326906, 0.11310692924126399, 0.13290611138570124, 0.12669085443839692, 0.13274477567452253, 0.163098646642606, 0.15626155223115282, 0.14822247476831255, 0.1123306807912923, 0.11110996922852087, 0.15505294331787225, 0.10985495199701176, 0.14819665487336925, 0.15970181227622945, 0.1426150315987661, 0.12561373676233134, 0.16641687961403467, 0.10593372765133276, 0.13931708876075116, 0.12566863828396854, 0.15455156858619618], [0.059447520133906924, 0.24371058074426794, 0.050921074833381305, 0.06569390971183857, 0.252461755032759, 0.056221801674853786, 0.22019735351653896, -0.14013058756081181, 0.243774556880545, 0.0506496569052869, 0.0657044455603676, 0.2525290710136459, 0.055907346794676606, 0.04495891644436398, 0.06049437651197038, 0.049976199112312214, 0.06345817225599117, 0.2202444571904101, -0.1399237786200919, -0.13991839112372897, -0.045467329494568005, -0.10255145121587823, 0.12284106450406607, 0.12279905001204196, -0.038307299468200176, 0.20058854375900154, -0.1384215571120059, -0.07892849247639062, 0.032242600369477, -0.16236794047306324, 0.20804938429329173, -0.07420232208054882, 0.03634994811483602, -0.15303458764067535, -0.11065994983866712, 0.18218842381978123, -0.08378757323533201, -0.07570374081656195, 0.027358585880783985, 0.18906218377720269, -0.07620000788433653, -0.07384993223979747, 0.031275454774248694, -0.1552731127967254, -0.04746896758967335, -0.10279776015846019, 0.122799049991009, 0.12275350866072306, -0.039967880047528374, 0.20055588701456145, -0.14289722255757986, -0.08767011531747712, -0.07919861963723748, 0.03240490363338092, -0.1625265413464847, 0.20801447859674263, -0.07451156239539894, 0.036490146870352874, -0.1531981315608028]]\n",
            "Explained variance:\n",
            " [2.3880209630152627, 0.7584997811733811]\n",
            "Mean: [0.44509183 0.58175159 0.44509714 0.4877193  0.58904155 0.44207239\n",
            " 0.61727273 0.6208864  0.58187721 0.44824235 0.48787759 0.58916816\n",
            " 0.44539337 0.44729716 0.48518205 0.44433043 0.48019315 0.61742518\n",
            " 0.62103966 0.58034873 0.47409879 0.46456138 0.5613864  0.56119971\n",
            " 0.47509734 0.58592604 0.53941138 0.50384173 0.42512661 0.58553361\n",
            " 0.59268308 0.51084357 0.42253664 0.59611971 0.51772359 0.54882384\n",
            " 0.51583909 0.4598941  0.42474406 0.55487663 0.49423541 0.43391439\n",
            " 0.42206863 0.57468173 0.45462819 0.46438958 0.56119971 0.56102021\n",
            " 0.45652385 0.58575475 0.52506536 0.56815088 0.50361401 0.42230027\n",
            " 0.58534173 0.59250943 0.51059615 0.42034145 0.59593079]\n",
            "Avg. Normalized Score:8.6%\n",
            "Avg. Error:0.0077\n",
            "Test predictions:\n",
            "[0.09101385 0.09160427 0.07620196 0.08979573 0.0918455  0.0908747\n",
            " 0.09417937 0.08281204 0.08895972 0.08773821]\n",
            "Avg. Normalized Score:12.3%\n",
            "Avg. Error:0.0102\n",
            "Difference of avg scores:3.70%\n",
            "\n",
            "Blender Train Scores then Test Scores:\n",
            "Avg. Normalized Score:6.7%\n",
            "Avg. Error:0.0059\n",
            "\n",
            "Avg. Normalized Score:11.5%\n",
            "Avg. Error:0.0097\n",
            "\n",
            "2024 RA/Batter Predictions:\n",
            "[0.09371158 0.09370001 0.09377352 0.09392626 0.09367187 0.09377322\n",
            " 0.09365859 0.09364055 0.09339253 0.09377342 0.0937663  0.09364554]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:493: UserWarning: X does not have valid feature names, but PCA was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:493: UserWarning: X does not have valid feature names, but PCA was fitted with feature names\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#R/PA Blender: 8.4, 2, avg. error: 0.0090, top 6, .845 to get 60 cols - 9.4\n",
        "#loaded_RF_model = pickle.load(open(\"RF_model R_PA\", \"rb\")) #\n",
        "#loaded_RF_model2 = pickle.load(open(\"RF_model R_PA2\", \"rb\")) #\n",
        "loaded_RF_model3 = pickle.load(open(\"RF_model R_PA3\", \"rb\")) #\n",
        "loaded_RF_model4 = pickle.load(open(\"RF_model R_PA4\", \"rb\")) #\n",
        "loaded_BR_model = pickle.load(open(\"BR_model R_PA\", \"rb\")) #\n",
        "loaded_BR_model2 = pickle.load(open(\"BR_model R_PA2\", \"rb\")) #\n",
        "\n",
        "R_PA = pd.read_csv('R_PA_inputs.csv')\n",
        "R_PA_scaled = scaler_R_PA.transform(R_PA)\n",
        "R_PA_scaled_MMS = scaler_R_PA_MMS.transform(R_PA)\n",
        "R_PA_PCA = (reduce_df(inputs, \"PCA\")[1]).transform(R_PA_scaled)\n",
        "R_PA_LLE = (reduce_df(inputs, \"\")[2]).transform(R_PA_scaled)\n",
        "R_PA_LLE_MMS = (reduce_df(inputs_MMS, \"\")[2]).transform(R_PA_scaled_MMS)\n",
        "\n",
        "preds = (loaded_BR_model.predict(inputs_MMS[0:10]), loaded_BR_model.predict(inputs_MMS[10:30]),\n",
        "         loaded_BR_model.predict(R_PA_scaled_MMS)) #9.2, 1.9 - 10.2 (plain)\n",
        "preds2 = GBR_model(inputs_LLE, Y, .0001, 0.01, 100, R_PA_LLE) #9.7, 2.1 - 10.8\n",
        "preds3 = (loaded_RF_model3.predict(inputs_LLE[0:10]), loaded_RF_model3.predict(inputs_LLE[10:30]),\n",
        "          loaded_RF_model3.predict(R_PA_LLE)) #9.3, 3.3 - 11 (plain)\n",
        "preds4 = (loaded_RF_model4.predict(inputs_LLE[0:10]), loaded_RF_model4.predict(inputs_LLE[10:30]),\n",
        "          loaded_RF_model4.predict(R_PA_LLE)) #9.5, 3.6 - 11.3 (poisson)\n",
        "preds5 = (loaded_BR_model2.predict(inputs[0:10]), loaded_BR_model2.predict(inputs[10:30]),\n",
        "          loaded_BR_model2.predict(R_PA_scaled)) #10.1, 2.8 - 11.5 (Boost)\n",
        "preds6 = GBR_model(inputs_PCA, Y, .0001, 0.01, 100, R_PA_PCA) #10.2, 2.9 - 11.7\n",
        "#preds7 = (loaded_RF_model.predict(inputs_MMS[0:10]), loaded_RF_model.predict(inputs_MMS[10:30])) #10.2, 3.9, MMS - 12.2 (abs or poisson)#\n",
        "#preds8 = (loaded_RF_model2.predict(inputs_MMS[0:10]), loaded_RF_model2.predict(inputs_MMS[10:30])) #10.2, 4.1, MMS - 12.3 (abs or poisson)\n",
        "#preds9 = RLE_Model(inputs_MMS, Y, \"Elastic\", inputs_MMS) #11.2, 2.6, MMS - 12.5\n",
        "#preds10 = RLE_Model(inputs_MMS_PCA, Y, \"Elastic\", inputs_MMS_PCA) #11.3, 2.5, MMS - 12.6\n",
        "\n",
        "train_preds = (preds[1] + preds2[1] + preds3[1] + preds4[1] + preds5[1] + preds6[1])/6\n",
        "\n",
        "test_preds = (preds[0] + preds2[0] + preds3[0] + preds4[0] + preds5[0] + preds6[0])/6\n",
        "\n",
        "R_PA_preds_2024 = (preds[2] + preds2[2] + preds3[2] + preds4[2] + preds5[2] + preds6[2])/6\n",
        "\n",
        "print(\"Blender Train Scores then Test Scores:\")\n",
        "Scores(Y[10:30], train_preds, Y)\n",
        "print()\n",
        "Scores(Y[0:10], test_preds, Y)\n",
        "print()\n",
        "print(\"2024 RA/GM Predictions:\")\n",
        "print(R_PA_preds_2024)\n",
        "R_PA_preds_2024_df = pd.DataFrame(R_PA_preds_2024)\n",
        "R_PA_preds_2024_df.to_excel(\"2024 R_PA Predictions.xlsx\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "akmTHnXzNR9X",
        "outputId": "2869400c-6bee-4919-cb4f-d8750afc261c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Principal axes:\n",
            " [[0.07237482520113063, 0.06514312738305279, 0.07381719597320083, 0.07432620564417702, 0.0672207841239294, 0.07952315767037843, 0.11200479552159832, 0.13829796327052316, 0.14061597360048073, 0.13588421644327958, 0.12355302305804197, 0.13912302603588578, 0.13411430368859897, 0.1339751814328834, 0.14583931530258784, 0.1251094654585108, 0.13112464369879973, 0.08816204286497996, 0.13020262822110037, 0.11819463631884258, 0.13111618253250967, 0.1345457746026534, 0.1322313756971539, 0.1472380640477702, 0.1270081997725471, 0.1249736730592046, 0.14668570703234063, 0.15011702489974463, 0.14900004922727958, 0.14510595259660664, 0.15029323644850717, 0.1338563709769622, 0.1379376437110802, 0.12450167168804427, 0.11534906941013527, 0.14728792264434443, 0.1454804075737005, 0.14187096115391135, 0.14649047896439035, 0.13498973932349095, 0.13881546960697794, 0.1151047347971632, 0.1263345480912702, 0.13658730363633104, 0.138933002521503, 0.134826042812561, 0.12294367130433052, 0.13744807355871289, 0.13288027044150458, 0.14537303600449059, 0.12457093213551697, 0.1292521758451422, 0.12906391080006505, 0.11752032980833421, 0.12926722420191067, 0.13360361505497875, 0.13119185544143874, 0.14707685517027028, 0.12655099566403, 0.14622097308739906], [0.07265255584116732, 0.0929579354351816, -0.05968872488998241, 0.07078289460315666, 0.09077967733963951, -0.06061211004505681, 0.2010667911900369, 0.14285248999740766, 0.06904284212224593, 0.14013245894497958, 0.024565734229892437, 0.14364486485064729, 0.06944499935076931, -0.16890500191426472, -0.10773066751702538, -0.15387761515100482, 0.1753276850980656, 0.24361177352947938, 0.16397448424039537, 0.04140016892716536, 0.1740392589160622, -0.1560827881074717, -0.12020942285653839, -0.08940611423460992, -0.14885157462308762, -0.12722612937155955, -0.1070239717639548, 0.07837724935610733, 0.02525106972930561, 0.08959536912519724, 0.07997789642321823, -0.177827088117543, -0.15995133098055866, -0.17162718010632227, -0.09118333479778325, 0.10751545579960656, 0.04572911719880763, 0.11168164669739471, 0.10771884216527039, -0.168791282362656, -0.14754385520133415, -0.07723954093812453, -0.154377767661353, 0.1466596233580547, 0.0708171482318701, 0.14329137802068753, 0.025760268186004576, 0.14736935455691713, -0.17096100808990022, -0.10888910553888448, -0.1548401188557898, 0.17994386917976146, 0.16766592458916102, 0.04289541433492185, 0.1784538801074966, -0.1583733233087195, -0.1217628048036537, -0.09042700858453738, -0.14994373924022364, -0.10811956264679191]]\n",
            "Explained variance:\n",
            " [40.85142217821765, 9.302928128109965]\n",
            "Mean: [ 2.37587727e-15  3.87467836e-15  1.61259894e-15 -8.43769499e-16\n",
            "  2.99760217e-16  4.41313652e-15 -5.55111512e-17  5.10702591e-16\n",
            " -2.05946371e-15 -2.77555756e-16  9.65894031e-16  1.42108547e-15\n",
            " -1.03805853e-15 -2.97123437e-15 -2.37587727e-15 -6.00908212e-16\n",
            "  1.60982339e-15 -4.64073224e-15 -3.65263375e-15 -1.57651669e-15\n",
            " -3.20854454e-15 -3.16387541e-15  1.10744747e-15  3.60267371e-15\n",
            "  2.27595720e-16  2.20379270e-15 -1.23789867e-15  3.27515792e-15\n",
            "  1.03805853e-15 -8.32667268e-16 -2.88657986e-15  2.35367281e-15\n",
            " -1.83186799e-16 -1.96925809e-15 -2.32730502e-15  2.10942375e-16\n",
            "  4.21884749e-15 -5.77315973e-16 -7.99360578e-16  1.02695630e-16\n",
            " -3.04201109e-15 -6.67521594e-16  2.77555756e-17  2.30926389e-15\n",
            "  1.62092562e-15  3.88578059e-16 -2.63122857e-15 -2.45359288e-15\n",
            "  6.29635233e-15 -1.37667655e-15 -1.07969189e-15  7.21644966e-16\n",
            " -2.76445533e-15 -1.04360964e-15 -2.95319325e-15 -2.00525360e-15\n",
            " -1.76803017e-15  9.43689571e-17 -1.07414078e-15 -1.97064587e-15]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:493: UserWarning: X does not have valid feature names, but PCA was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:493: UserWarning: X does not have valid feature names, but LocallyLinearEmbedding was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:493: UserWarning: X does not have valid feature names, but LocallyLinearEmbedding was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:493: UserWarning: X does not have valid feature names, but BaggingRegressor was fitted with feature names\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Avg. Normalized Score:7.6%\n",
            "Avg. Error:0.0078\n",
            "Test predictions:\n",
            "[0.12723086 0.12326176 0.12723086 0.11645235 0.11394414 0.14629652\n",
            " 0.12723086 0.12353472 0.13751158 0.11394414]\n",
            "Avg. Normalized Score:9.7%\n",
            "Avg. Error:0.0108\n",
            "Difference of avg scores:2.08%\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:493: UserWarning: X does not have valid feature names, but BaggingRegressor was fitted with feature names\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Avg. Normalized Score:7.2%\n",
            "Avg. Error:0.0076\n",
            "Test predictions:\n",
            "[0.13577919 0.11133737 0.13577919 0.11133737 0.13577919 0.13577919\n",
            " 0.11613258 0.11613258 0.1456435  0.1147916 ]\n",
            "Avg. Normalized Score:10.2%\n",
            "Avg. Error:0.0103\n",
            "Difference of avg scores:2.92%\n",
            "\n",
            "Blender Train Scores then Test Scores:\n",
            "Avg. Normalized Score:6.4%\n",
            "Avg. Error:0.0064\n",
            "\n",
            "Avg. Normalized Score:8.4%\n",
            "Avg. Error:0.0090\n",
            "\n",
            "2024 RA/GM Predictions:\n",
            "[0.13326052 0.11945574 0.11729997 0.11686141 0.11271409 0.11774292\n",
            " 0.13344949 0.12227888 0.11648336 0.13031987 0.12963305 0.11882093]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#RA/GM Blender: 13.6, .7, avg. error: .4, .855 to get 60 cols for RA/GM for standard - 14.2\n",
        "inputs2 = inputs\n",
        "inputs2_PCA = inputs_PCA\n",
        "inputs2_LLE = inputs_LLE\n",
        "\n",
        "RA_GM = pd.read_csv('RA_GM.csv')\n",
        "RA_GM_scaled = scaler_RA_GM.transform(RA_GM)\n",
        "RA_GM_scaled_MMS = scaler_RA_GM_MMS.transform(RA_GM)\n",
        "RA_GM_PCA = (reduce_df(inputs2, \"PCA\")[1]).transform(RA_GM_scaled)\n",
        "RA_GM_LLE_MMS = (reduce_df(inputs_MMS, \"\")[2]).transform(RA_GM_scaled_MMS)\n",
        "\n",
        "#loaded_SGD_model = pickle.load(open(\"SGD_model RA_GM\", \"rb\")) #\n",
        "#loaded_SGD_model2 = pickle.load(open(\"SGD_model RA_GM2\", \"rb\"))\n",
        "\n",
        "preds = SVM_models(inputs2_PCA, Y, \"rbf\", .6, 100, RA_GM_PCA) #13.7, .4, .6 - 13.9\n",
        "preds2 = RLE_Model(inputs_MMS, Y, \"Lasso\", RA_GM_scaled_MMS) #13.6, 1.9, MMS - 14.6\n",
        "preds3 = SVM_models(inputs_MMS_LLE, Y, \"linear\", .9, 100, RA_GM_LLE_MMS) #14.5, .5, .9, MMS - 14.8\n",
        "#preds4 = SVM_models(inputs_MMS_LLE, Y, \"rbf\", .9, 100, RA_GM_LLE_MMS) #14.5, .6, .9, MMS - 14.8\n",
        "#preds5 = RLE_Model(inputs2, Y, \"Lasso\", RA_GM_scaled) #14.8, .8 - 15.2\n",
        "#preds6 = RLE_Model(inputs2_PCA, Y, \"Lasso\", RA_GM_PCA) #15, .9 - 15.5\n",
        "#preds7 = RLE_Model(inputs_PCA, Y, \"Lasso\", inputs_PCA) #14.9, 1.1, MMS - 15.5\n",
        "#preds8 = (loaded_SGD_model2.predict(inputs_LLE_10_15[0:10]), loaded_SGD_model2.predict(inputs_LLE_10_15[10:30])) #15.5, .2, MMS, adjs: 10, 15, - 15.6\n",
        "#preds9 = (loaded_SGD_model.predict(inputs2_LLE_10_15[0:10]), loaded_SGD_model.predict(inputs2_LLE_10_15[10:30])) #15.7, .4, adjs: 10, 15 - 15.9\n",
        "#preds10 = RLE_Model(inputs2_LLE, Y, \"Elastic\", inputs_LLE) #15.4, 1.2 - 16\n",
        "\n",
        "train_preds = (preds[1] + preds2[1])/2\n",
        "\n",
        "test_preds = (preds[0] + preds2[0])/2\n",
        "\n",
        "RA_GM_preds_2024 = (preds[2] + preds2[2])/2\n",
        "\n",
        "print(\"Blender Train Scores then Test Scores:\")\n",
        "Scores(Y[10:30], train_preds, Y)\n",
        "print()\n",
        "Scores(Y[0:10], test_preds, Y)\n",
        "print()\n",
        "print(\"2024 RA/GM Predictions:\")\n",
        "print(RA_GM_preds_2024)\n",
        "RA_GM_preds_2024_df = pd.DataFrame(RA_GM_preds_2024)\n",
        "RA_GM_preds_2024_df.to_excel(\"2024 RA_GM Predictions.xlsx\")"
      ],
      "metadata": {
        "id": "5RhajCl1K4BQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "34280320-6303-4963-b89b-a2735aa0a764",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Principal axes:\n",
            " [[-0.0749310197257101, -0.07348300991999297, 0.13655845782264062, 0.14120154339359087, 0.1386949819012305, 0.13728544524917666, 0.14012530875872137, 0.14009458460531993, 0.1361369728256285, 0.1406111088563674, 0.13925669879342803, 0.07003426006384814, 0.13739578027406693, 0.14033504177640152, 0.14024862628322665, 0.13624339271285218, 0.14080961581298196, 0.13940250410837002, 0.13151702135010884, 0.13106028460846156, 0.13900527562295922, 0.1410108670092537, 0.1410342706609554, 0.13802022008376005, 0.1417001518160689, 0.14053681830397952, 0.0712359058416632, 0.10720152969161156, 0.09842559539243212, 0.11234589784014794, 0.14221969588998934, 0.07810317957254306, 0.1408511491302939, 0.14300878039899448, 0.1412567974196842, 0.07949646642882548, 0.14119893034828243, 0.14235625351040393, 0.13565101840680868, 0.0942182931154841, 0.13860043225534371, 0.14261719694912917, 0.14023049070580143, 0.14203369195096063, 0.13555423179802653, 0.13814817125439163, 0.14179395405649625, 0.14069393247132203, 0.14162070040481875, 0.1063361520756579, 0.09736721683947419, 0.11157398705126385, 0.142201143716687, 0.07692718830450089, 0.14072884680280037, 0.14298905385714533, 0.14124585713555018, 0.0783601312221357, 0.14109111045902517, 0.14235116461210864], [-0.010827526173839825, -0.009117901172924419, -0.12640106349871721, -0.010203965405193235, -0.09340299621379008, -0.12053713358811345, -0.0009281073833193311, -0.0841687062867403, -0.127712911335707, -0.010202033418659644, -0.09500066189176845, 0.263975914469518, -0.11962327141187899, -0.0002228092854727927, -0.08300920755094607, -0.12675826235413534, -0.009448539520303053, -0.09376260564240098, -0.04845693493079873, -0.11731728996582565, -0.1040562399002374, 0.018218167581652116, -0.061322382645317745, -0.11122780209805325, 0.009447755564403472, -0.07187533439419647, 0.26376980352139856, 0.20862003203949722, 0.2558215779313984, 0.19225293393446274, -0.07494093864409417, 0.2600178159304837, 0.045037670055227984, -0.025443856154180888, -0.08158145915586472, 0.2510210639844906, 0.037626375158876485, -0.034569848841077055, -0.0024090154582324004, 0.27904335635379696, -0.06543417213811356, -0.05980010484348144, 0.060526839300442205, -0.006424037195619413, -0.010062591092538337, -0.07403555544535653, -0.06635230993395627, 0.053548186574384864, -0.015170211851521072, 0.20929622860255465, 0.25624856012373737, 0.19262125141783382, -0.07576505376894399, 0.2618907246988134, 0.044513902586081525, -0.026339981700575988, -0.08244556257754698, 0.2534643444921913, 0.037070321504683666, -0.035530406208623716]]\n",
            "Explained variance:\n",
            " [48.600720567562576, 6.564225206366155]\n",
            "Mean: [ 2.04003481e-15  2.95319325e-15 -2.82759927e-17  1.07691633e-15\n",
            " -1.62647673e-15  2.65933109e-16  2.66453526e-16 -1.21569421e-15\n",
            "  3.87363752e-16 -3.33066907e-17 -1.13797860e-16 -1.66533454e-16\n",
            "  2.69055611e-16 -6.43929354e-16  8.32667268e-17  1.41900380e-16\n",
            "  3.33066907e-16 -8.90953977e-16 -6.93889390e-19 -3.46944695e-18\n",
            "  3.62904151e-16  9.21485110e-16 -4.94049246e-16  8.31279490e-16\n",
            " -3.10862447e-16 -8.04911693e-16 -1.75415238e-15  5.10702591e-16\n",
            " -2.05946371e-15  1.42108547e-15 -1.72084569e-16 -2.01783035e-15\n",
            "  4.32986980e-16  3.10862447e-16  4.11476409e-16 -1.20597976e-15\n",
            " -1.55431223e-16  2.66453526e-16 -6.93889390e-17  1.03805853e-15\n",
            "  1.30589983e-15  8.78463968e-16  1.44328993e-16 -3.66373598e-16\n",
            "  1.70696790e-16 -2.44249065e-16  8.72912853e-16  1.13242749e-15\n",
            "  4.10782519e-16  2.30926389e-15  1.62092562e-15 -2.45359288e-15\n",
            " -3.89965837e-16  1.09634524e-15 -3.33066907e-17 -6.93889390e-16\n",
            " -1.14214194e-15  1.88460358e-15 -3.33066907e-17  6.32827124e-16]\n",
            "Avg. Normalized Score:13.3%\n",
            "Avg. Error:0.4462\n",
            "Test predictions:\n",
            "[3.56926101 3.57527222 3.68040259 3.48814895 3.58068673 3.64569776\n",
            " 3.41236637 3.57686241 3.61170458 3.60146109]\n",
            "Avg. Normalized Score:13.7%\n",
            "Avg. Error:0.4378\n",
            "Difference of avg scores:0.40%\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:493: UserWarning: X does not have valid feature names, but PCA was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:493: UserWarning: X does not have valid feature names, but LocallyLinearEmbedding was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.295e-02, tolerance: 7.251e-04\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.674e-02, tolerance: 7.251e-04\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.563e-02, tolerance: 7.251e-04\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.085e-01, tolerance: 7.251e-04\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.526e-01, tolerance: 7.251e-04\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.421e-01, tolerance: 7.251e-04\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.252e-01, tolerance: 7.251e-04\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.819e-02, tolerance: 7.251e-04\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.446e-02, tolerance: 7.251e-04\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.223e-02, tolerance: 7.251e-04\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.462e-02, tolerance: 7.251e-04\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.249e-02, tolerance: 7.251e-04\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.432e-02, tolerance: 7.251e-04\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.065e-03, tolerance: 7.251e-04\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.718e-03, tolerance: 7.251e-04\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.598e-03, tolerance: 7.251e-04\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.260e-03, tolerance: 7.251e-04\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.423e-03, tolerance: 7.251e-04\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.051e-04, tolerance: 7.251e-04\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.105e-04, tolerance: 4.125e-04\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.157e-03, tolerance: 4.125e-04\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.429e-02, tolerance: 4.125e-04\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.667e-03, tolerance: 4.125e-04\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.796e-02, tolerance: 4.125e-04\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.143e-01, tolerance: 4.125e-04\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.232e-01, tolerance: 4.125e-04\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.844e-02, tolerance: 4.125e-04\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.626e-02, tolerance: 4.125e-04\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.320e-02, tolerance: 4.125e-04\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.804e-02, tolerance: 4.125e-04\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.799e-02, tolerance: 4.125e-04\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.145e-02, tolerance: 4.125e-04\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.257e-03, tolerance: 4.125e-04\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.584e-03, tolerance: 4.125e-04\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.890e-03, tolerance: 4.125e-04\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.819e-03, tolerance: 4.125e-04\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.145e-03, tolerance: 4.125e-04\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.196e-04, tolerance: 4.125e-04\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.522e-04, tolerance: 4.125e-04\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.943e-03, tolerance: 6.936e-04\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.341e-03, tolerance: 6.936e-04\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.614e-03, tolerance: 6.936e-04\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.463e-02, tolerance: 6.936e-04\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.236e-02, tolerance: 6.936e-04\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.272e-02, tolerance: 6.936e-04\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.085e-02, tolerance: 6.936e-04\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.025e-01, tolerance: 6.936e-04\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.022e-01, tolerance: 6.936e-04\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.371e-02, tolerance: 6.936e-04\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.521e-02, tolerance: 6.936e-04\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.838e-02, tolerance: 6.936e-04\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.532e-02, tolerance: 6.936e-04\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.494e-02, tolerance: 6.936e-04\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.712e-02, tolerance: 6.936e-04\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.165e-02, tolerance: 6.936e-04\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.795e-03, tolerance: 6.936e-04\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.142e-03, tolerance: 6.936e-04\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.356e-03, tolerance: 6.936e-04\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.173e-03, tolerance: 6.936e-04\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.399e-03, tolerance: 6.936e-04\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.954e-04, tolerance: 6.936e-04\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.247e-03, tolerance: 7.226e-04\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.292e-02, tolerance: 7.226e-04\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.695e-02, tolerance: 7.226e-04\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.132e-02, tolerance: 7.226e-04\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.271e-02, tolerance: 7.226e-04\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.019e-01, tolerance: 7.226e-04\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.075e-02, tolerance: 7.226e-04\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.934e-02, tolerance: 7.226e-04\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.179e-02, tolerance: 7.226e-04\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.888e-02, tolerance: 7.226e-04\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.943e-02, tolerance: 7.226e-04\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.265e-02, tolerance: 7.226e-04\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.086e-03, tolerance: 7.226e-04\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.143e-03, tolerance: 7.226e-04\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.242e-03, tolerance: 7.226e-04\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.035e-03, tolerance: 7.226e-04\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.280e-03, tolerance: 7.226e-04\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.045e-04, tolerance: 7.226e-04\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.391e-03, tolerance: 6.816e-04\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.790e-03, tolerance: 6.816e-04\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.219e-03, tolerance: 6.816e-04\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.342e-03, tolerance: 6.816e-04\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.403e-02, tolerance: 6.816e-04\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.018e-01, tolerance: 6.816e-04\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.180e-01, tolerance: 6.816e-04\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.827e-02, tolerance: 6.816e-04\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.404e-02, tolerance: 6.816e-04\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.160e-02, tolerance: 6.816e-04\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.402e-02, tolerance: 6.816e-04\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.201e-02, tolerance: 6.816e-04\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.386e-02, tolerance: 6.816e-04\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.813e-03, tolerance: 6.816e-04\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.589e-03, tolerance: 6.816e-04\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.533e-03, tolerance: 6.816e-04\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.228e-03, tolerance: 6.816e-04\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.402e-03, tolerance: 6.816e-04\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.820e-04, tolerance: 6.816e-04\n",
            "  model = cd_fast.enet_coordinate_descent(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Avg. Normalized Score:15.6%\n",
            "Avg. Error:0.5105\n",
            "Test predictions:\n",
            "[3.38540024 3.42431679 3.41404403 3.47201129 3.40907784 3.71134451\n",
            " 3.54003174 3.49538779 3.49942655 3.44941731]\n",
            "Avg. Normalized Score:13.6%\n",
            "Avg. Error:0.4504\n",
            "Difference of avg scores:-1.94%\n",
            "\n",
            "Avg. Normalized Score:15.1%\n",
            "Avg. Error:0.4940\n",
            "Test predictions:\n",
            "[3.53039111 3.51097978 3.49051738 3.52981129 3.47307496 3.47649143\n",
            " 3.63050779 3.49849966 3.49301577 3.47470326]\n",
            "Avg. Normalized Score:14.5%\n",
            "Avg. Error:0.4780\n",
            "Difference of avg scores:-0.54%\n",
            "\n",
            "Blender Train Scores then Test Scores:\n",
            "Avg. Normalized Score:14.3%\n",
            "Avg. Error:0.4778\n",
            "\n",
            "Avg. Normalized Score:13.6%\n",
            "Avg. Error:0.4441\n",
            "\n",
            "2024 RA/GM Predictions:\n",
            "[3.50679763 3.52230502 3.51173108 3.49155934 3.50436812 3.42158393\n",
            " 3.5010521  3.47324401 3.55236621 3.42177794 3.3233635  3.49418503]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:493: UserWarning: X does not have valid feature names, but LassoCV was fitted with feature names\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#R/GM Blender: 11, .8, avg error: .4, .83 thresh to get 61 cols - 11.4\n",
        "R_GM = pd.read_csv('R_GM.csv')\n",
        "R_GM_scaled = scaler_R_GM.transform(R_GM)\n",
        "R_GM_PCA = (reduce_df(inputs, \"PCA\")[1]).transform(R_GM_scaled)\n",
        "\n",
        "loaded_RF_model = pickle.load(open(\"RF_model R_GM\", \"rb\")) #\n",
        "#loaded_RF_model2 = pickle.load(open(\"RF_model R_GM2\", \"rb\")) #\n",
        "#loaded_SGD_model = pickle.load(open(\"SGD_model R_GM\", \"rb\")) #12.4, 3.2 - 14\n",
        "#loaded_SGD_model2 = pickle.load(open(\"SGD_model R_GM2\", \"rb\")) #13.3, 2.6 - 14.6\n",
        "\n",
        "preds = GBR_model(inputs_PCA, Y, .0001, 0.0055, 97, R_GM_PCA) #11.2, .1, .0055, 97 - 11.3\n",
        "preds2 = (loaded_RF_model.predict(inputs[0:10]), loaded_RF_model.predict(inputs[10:30]), loaded_RF_model.predict(R_GM_scaled)) #10.7, 4.4, 98 - 12.9 (abs RF)\n",
        "#preds3 = (loaded_RF_model2.predict(inputs[0:10]), loaded_RF_model2.predict(inputs[10:30])) #11.2, 4.6, 80 - 13.5 (standard RF)\n",
        "#preds4 = SVM_models(inputs_LLE, Y, \"poly\", .5, 100, inputs_LLE) #13.5, .3, .5 - 13.7\n",
        "preds5 = SVM_models(inputs, Y, \"poly\", 1.2, 100, R_GM_scaled) #13.3, 1.2, 1.2 - 13.9\n",
        "#preds6 = (loaded_SGD_model.predict(inputs_PCA[0:10]), loaded_SGD_model.predict(inputs_PCA[10:30])) #12.4, 3.2 - 14, PCA\n",
        "#preds7 = GBR_model(inputs, Y, .000, 0.0008, 200, inputs) #14, 0, 0.0008, 200 - 14\n",
        "#preds8 = RLE_Model(inputs, Y, \"Elastic\", inputs) #12.9, 2.4 - 14.1\n",
        "#preds9 = RLE_Model(inputs_MMS, Y, \"Ridge\", inputs_MMS) #13.2, 2.3, MMS - 14.4\n",
        "#preds10 = SVM_models(inputs_LLE, Y, \"rbf\", .1, 100, inputs_LLE) #13.6, 1.5, .3 - 14.4\n",
        "\n",
        "train_preds = (preds[1] + preds2[1] + preds5[1])/3\n",
        "\n",
        "test_preds = (preds[0] + preds2[0] + preds5[0])/3\n",
        "\n",
        "R_GM_preds_2024 = (preds[2] + preds2[2] + preds5[2])/3\n",
        "\n",
        "print(\"Blender Train Scores then Test Scores:\")\n",
        "Scores(Y[10:30], train_preds, Y)\n",
        "print()\n",
        "Scores(Y[0:10], test_preds, Y)\n",
        "print()\n",
        "print(\"2024 R/GM Predictions:\")\n",
        "print(R_GM_preds_2024)\n",
        "R_GM_preds_2024_df = pd.DataFrame(R_GM_preds_2024)\n",
        "R_GM_preds_2024_df.to_excel(\"2024 R_GM Predictions.xlsx\")"
      ],
      "metadata": {
        "id": "UTf6zpfk2PAk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "outputId": "a3477044-d08d-41fb-c9ef-92199e26af99"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Principal axes:\n",
            " [[0.06897292284290153, -0.059744085603228025, 0.07577551361839686, 0.10047164700424138, 0.0814169430227213, 0.10372637886386546, -0.03670187278073265, 0.12234300642038247, 0.13271501630945506, 0.12182225207005858, 0.12320277617489492, 0.14669787531192746, 0.143929052073579, 0.09157284933570577, 0.15235718925000852, 0.13999499534882945, 0.11276414292530366, 0.12488786905684617, 0.1144141340995752, 0.11298192854890482, 0.1515290944539778, 0.14650754079090028, 0.1414453303018706, 0.15244483977240111, 0.1379996550349826, 0.11359359378645047, 0.12926786212709687, 0.134505481650397, 0.15152538786754105, 0.1421067894838163, 0.13260358820008264, 0.13660645136429164, 0.14440291860832646, 0.14687614434007418, 0.09644179442982644, 0.14001350477775412, 0.12896234182367353, 0.12770310272066052, 0.14485681127318747, 0.14678715951064666, 0.13935051516265623, 0.1282466318260486, 0.13193620486304172, 0.12037113567761011, 0.13095850876621282, 0.12056479767633299, 0.12127458399859299, 0.14586568388495075, 0.14301914702697865, 0.08994083235660497, 0.15205233146990918, 0.13904649893991203, 0.11056924759375782, 0.1130368969071351, 0.1108286620974575, 0.14586146320776938, 0.14065235198668827, 0.15244017450784977, 0.13714615159730129, 0.1288547027881256, 0.1511811910921234], [-0.03200065932494756, 0.140200248626974, -0.07705608487503886, -0.06370542779155564, -0.07688783379835207, -0.06293730693657178, 0.1470615024372184, 0.20243451131382598, 0.14484554715868583, 0.1745161843212623, 0.2015502301574238, -0.08972007926752833, -0.05964658098164048, -0.18037610992889452, -0.040112777175973786, -0.1132890370205063, 0.22511262313793395, 0.16006210570177445, 0.1903713665499195, 0.22193093924338794, -0.020527545453593546, -0.08136151635575967, -0.04682337939549024, -0.027294608909178276, -0.10608467231032398, -0.02151972232438135, -0.09256008332479576, -0.05717155112606911, -0.03691285548899841, 0.10183931291850935, 0.1291893878249648, 0.14160603119319928, -0.10394461601803981, -0.08382775012925908, -0.17859016095548053, -0.12378125182681247, -0.05699775220335034, 0.14448325251030641, -0.09861553222981836, -0.07567213656553945, -0.1192418892337154, -0.04909640198749381, -0.09878381252333313, 0.2069952467361171, 0.1472029313131103, 0.1780175073117155, 0.2059794647758819, -0.09045071474891327, -0.05981695530544038, -0.18104484402795337, -0.039981914733522306, -0.114487534222087, 0.23035592072138725, 0.19429854494238374, 0.22691029946864183, -0.08214397274653576, -0.04680145836947258, -0.02689748116914397, -0.10741525162435234, -0.09307900781762381, -0.03670872459542526]]\n",
            "Explained variance:\n",
            " [41.46110735167741, 9.81453753697844]\n",
            "Mean: [-6.53643806e-15 -3.71924713e-16  1.61259894e-15 -7.68829445e-15\n",
            "  4.41313652e-15  1.37057032e-14  2.16493490e-16  5.10702591e-16\n",
            " -2.05946371e-15 -2.77555756e-16  1.42108547e-15 -2.97123437e-15\n",
            "  2.82274204e-15 -8.88178420e-16 -2.37587727e-15 -1.66811009e-15\n",
            "  1.60982339e-15  4.38538095e-16 -3.65263375e-15 -3.20854454e-15\n",
            " -4.32986980e-15 -3.16387541e-15  1.10744747e-15  3.60267371e-15\n",
            "  2.94209102e-15  1.38222767e-15  2.27595720e-16  2.20379270e-15\n",
            " -1.23789867e-15  1.03805853e-15 -8.32667268e-16 -2.88657986e-15\n",
            "  2.35367281e-15 -1.83186799e-16 -2.77555756e-16 -1.96925809e-15\n",
            " -2.32730502e-15 -5.77315973e-16  1.02695630e-16 -3.04201109e-15\n",
            " -1.25732758e-15 -6.67521594e-16  2.43416398e-15  2.30926389e-15\n",
            "  1.62092562e-15  3.88578059e-16 -2.45359288e-15  6.29635233e-15\n",
            "  4.59077221e-15  7.82707232e-16 -1.37667655e-15  1.05748743e-15\n",
            "  7.21644966e-16 -2.76445533e-15 -2.95319325e-15 -2.00525360e-15\n",
            " -1.76803017e-15  9.43689571e-17  1.57651669e-15 -1.07414078e-15\n",
            " -1.97064587e-15]\n",
            "Avg. Normalized Score:11.1%\n",
            "Avg. Error:0.4468\n",
            "Test predictions:\n",
            "[4.74037215 4.3668129  4.97640453 4.3668129  4.74037215 4.97640453\n",
            " 4.3668129  4.3668129  5.05787463 4.5167126 ]\n",
            "Avg. Normalized Score:11.2%\n",
            "Avg. Error:0.4589\n",
            "Difference of avg scores:0.12%\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:493: UserWarning: X does not have valid feature names, but PCA was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:493: UserWarning: X does not have valid feature names, but RandomForestRegressor was fitted with feature names\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Avg. Normalized Score:14.6%\n",
            "Avg. Error:0.6069\n",
            "Test predictions:\n",
            "[5.37225343 4.24364327 5.40121143 4.48125383 4.75303746 5.14641749\n",
            " 4.81631746 4.85922489 4.88337659 4.72117842]\n",
            "Avg. Normalized Score:13.4%\n",
            "Avg. Error:0.5458\n",
            "Difference of avg scores:-1.22%\n",
            "\n",
            "Blender Train Scores then Test Scores:\n",
            "Avg. Normalized Score:10.2%\n",
            "Avg. Error:0.4184\n",
            "\n",
            "Avg. Normalized Score:11.0%\n",
            "Avg. Error:0.4463\n",
            "\n",
            "2024 R/GM Predictions:\n",
            "[4.34414049 4.66383183 4.58015293 4.30554959 4.25560897 4.36764829\n",
            " 4.87469559 4.18238712 4.31643173 4.51984075 4.37401902 4.26834602]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:493: UserWarning: X does not have valid feature names, but SVR was fitted with feature names\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Other:**"
      ],
      "metadata": {
        "id": "duGhmTO6gBij"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Check for collinearity\n",
        "import seaborn as sns\n",
        "#sns.pairplot(data_correlated_df2)\n",
        "\n",
        "corr = data_correlated_df2.corr()\n",
        "print(corr)"
      ],
      "metadata": {
        "id": "TyALuH9TPDdB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Get Pearson's correlation between 2 variables\n",
        "df_16.iloc[:,-3].corr(df_Playoffs_16.iloc[:,-2])"
      ],
      "metadata": {
        "id": "mmHqjfoXJx3W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_scaled_full = pd.DataFrame(scaler.fit_transform(df_NP), columns = df_NP.columns)\n",
        "data_correlated = correlation(data_scaled_full, .3, df['P%_Playoffs']) #top 11 stats\n",
        "data_correlated_df = pd.DataFrame(data_correlated)\n",
        "data_correlated_df2 = data_correlated_df.transpose() #Correlated inputs\n",
        "data_correlated_df2.columns"
      ],
      "metadata": {
        "id": "XJLnf0xcgh67"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_scaled_full"
      ],
      "metadata": {
        "id": "_ZOIx6-mgymi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_preds = (preds[1] + preds2[1] + preds3[1] + preds4[1] + preds5[1] + preds6[1] +\n",
        "               preds7[1] + preds8[1] + preds9[1] + preds10[1] + preds11[1] + preds12[1] +\n",
        "               preds13[1] + preds14[1] + preds15[1])/15\n",
        "\n",
        "test_preds = (preds[0] + preds2[0] + preds3[0] + preds4[0] + preds5[0] + preds6[0] +\n",
        "              preds7[0] + preds8[0] + preds9[0] + preds10[0] + preds11[0] + preds12[0] +\n",
        "              preds13[0] + preds14[0] + preds15[0])/15"
      ],
      "metadata": {
        "id": "6Eokk77DlqcO"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "l9zr28DwjQ6D",
        "WhvX9hZQi5U7",
        "duGhmTO6gBij"
      ],
      "mount_file_id": "1Vm8WmlMWl5qPkDh4nqbzn64vOM6ndqxq",
      "authorship_tag": "ABX9TyNPDbDFwtTBojKziQ4ek5JH",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}