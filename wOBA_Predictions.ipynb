{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "i7hsAFy2N-28",
        "_E3ua48mOCTy"
      ],
      "authorship_tag": "ABX9TyO4JwUj61tg75GkXPuJF8m+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ConorD28/Baseball/blob/main/wOBA_Predictions.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Load/Combine Data**"
      ],
      "metadata": {
        "id": "i7hsAFy2N-28"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0CfKtyaf0DYt"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import math\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "df_13 = pd.read_csv('2013 Hitters.csv')\n",
        "df_14 = pd.read_csv('2014 Hitters.csv')\n",
        "df_15 = pd.read_csv('2015 Hitters.csv')\n",
        "df_16 = pd.read_csv('2016 Hitters.csv')\n",
        "df_17 = pd.read_csv('2017 Hitters.csv')\n",
        "df_18 = pd.read_csv('2018 Hitters.csv')\n",
        "df_19 = pd.read_csv('2019 Hitters.csv')\n",
        "df_20 = pd.read_csv('2020 Hitters.csv')\n",
        "df_21 = pd.read_csv('2021 Hitters.csv')\n",
        "df_22 = pd.read_csv('2022 Hitters.csv')\n",
        "df_23 = pd.read_csv('2023 Hitters.csv')\n",
        "df_24 = pd.read_csv('2024 Hitters.csv')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_15['Year'] = 2015\n",
        "df_16['Year'] = 2016\n",
        "df_17['Year'] = 2017\n",
        "df_18['Year'] = 2018\n",
        "df_21['Year'] = 2021\n",
        "df_22['Year'] = 2022\n",
        "df_23['Year'] = 2023\n",
        "df_24['Year'] = 2024"
      ],
      "metadata": {
        "id": "XlJDuw9dKCOY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hitters_15 = pd.merge(df_15, df_14, how = 'left', on = 'Player', suffixes = (None, '_prev'))\n",
        "hitters_15 = pd.merge(hitters_15, df_13, how = 'left', on = 'Player', suffixes = (None, '_2_yrs_prev'))\n",
        "hitters_15 = pd.merge(hitters_15, df_16[['Player', 'PA', 'wOBA']], how = 'inner', on = 'Player', suffixes = (None, '_next'))\n",
        "hitters_15 = hitters_15[(hitters_15['PA'] >=400) & (hitters_15['PA_next'] >=400)]\n",
        "\n",
        "hitters_16 = pd.merge(df_16, df_15, how = 'left', on = 'Player', suffixes = (None, '_prev'))\n",
        "hitters_16 = pd.merge(hitters_16, df_14, how = 'left', on = 'Player', suffixes = (None, '_2_yrs_prev'))\n",
        "hitters_16 = pd.merge(hitters_16, df_17[['Player', 'PA', 'wOBA']], how = 'inner', on = 'Player', suffixes = (None, '_next'))\n",
        "hitters_16 = hitters_16[(hitters_16['PA'] >=400) & (hitters_16['PA_next'] >=400)]\n",
        "\n",
        "hitters_17 = pd.merge(df_17, df_16, how = 'left', on = 'Player', suffixes = (None, '_prev'))\n",
        "hitters_17 = pd.merge(hitters_17, df_15, how = 'left', on = 'Player', suffixes = (None, '_2_yrs_prev'))\n",
        "hitters_17 = pd.merge(hitters_17, df_18[['Player', 'PA', 'wOBA']], how = 'inner', on = 'Player', suffixes = (None, '_next'))\n",
        "hitters_17 = hitters_17[(hitters_17['PA'] >=400) & (hitters_17['PA_next'] >=400)]\n",
        "\n",
        "hitters_18 = pd.merge(df_18, df_17, how = 'left', on = 'Player', suffixes = (None, '_prev'))\n",
        "hitters_18 = pd.merge(hitters_18, df_16, how = 'left', on = 'Player', suffixes = (None, '_2_yrs_prev'))\n",
        "hitters_18 = pd.merge(hitters_18, df_19[['Player', 'PA', 'wOBA']], how = 'inner', on = 'Player', suffixes = (None, '_next'))\n",
        "hitters_18 = hitters_18[(hitters_18['PA'] >=400) & (hitters_18['PA_next'] >=400)]\n",
        "\n",
        "hitters_21 = pd.merge(df_21, df_20, how = 'left', on = 'Player', suffixes = (None, '_prev'))\n",
        "hitters_21 = pd.merge(hitters_21, df_19, how = 'left', on = 'Player', suffixes = (None, '_2_yrs_prev'))\n",
        "hitters_21 = pd.merge(hitters_21, df_22[['Player', 'PA', 'wOBA']], how = 'inner', on = 'Player', suffixes = (None, '_next'))\n",
        "hitters_21 = hitters_21[(hitters_21['PA'] >=400) & (hitters_21['PA_next'] >=400)]\n",
        "\n",
        "hitters_22 = pd.merge(df_22, df_21, how = 'left', on = 'Player', suffixes = (None, '_prev'))\n",
        "hitters_22 = pd.merge(hitters_22, df_20, how = 'left', on = 'Player', suffixes = (None, '_2_yrs_prev'))\n",
        "hitters_22 = pd.merge(hitters_22, df_23[['Player', 'PA', 'wOBA']], how = 'inner', on = 'Player', suffixes = (None, '_next'))\n",
        "hitters_22 = hitters_22[(hitters_22['PA'] >=400) & (hitters_22['PA_next'] >=400)]\n",
        "\n",
        "hitters_23 = pd.merge(df_23, df_22, how = 'left', on = 'Player', suffixes = (None, '_prev'))\n",
        "hitters_23 = pd.merge(hitters_23, df_21, how = 'left', on = 'Player', suffixes = (None, '_2_yrs_prev'))\n",
        "hitters_23 = pd.merge(hitters_23, df_24[['Player', 'PA', 'wOBA']], how = 'inner', on = 'Player', suffixes = (None, '_next'))\n",
        "hitters_23 = hitters_23[(hitters_23['PA'] >=400) & (hitters_23['PA_next'] >=400)]\n",
        "\n",
        "hitters_24 = pd.merge(df_24, df_23, how = 'left', on = 'Player', suffixes = (None, '_prev'))\n",
        "hitters_24 = pd.merge(hitters_24, df_22, how = 'left', on = 'Player', suffixes = (None, '_2_yrs_prev'))\n",
        "\n",
        "hitters_all = pd.concat([hitters_15, hitters_16, hitters_17, hitters_18, hitters_21, hitters_22, hitters_23, hitters_24])\n",
        "hitters_all.to_csv('hitters_400_PA.csv')"
      ],
      "metadata": {
        "id": "wlTZ6UvZ2uK7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hitters_24 = pd.read_csv('2024_400_PA_Players.csv')\n",
        "hitters_24['Year'] = 2024\n",
        "hitters_24['ID'] = hitters_24['MLB_ID'].astype(str) + hitters_24['Year'].astype(str)\n",
        "df_23 = pd.read_csv('2023 Hitters.csv')\n",
        "df_22 = pd.read_csv('2022 Hitters.csv')\n",
        "df_22['Year'] = 2022\n",
        "df_23['Year'] = 2023\n",
        "\n",
        "hitters_24 = pd.merge(hitters_24, df_23, how = 'left', on = 'Player', suffixes = (None, '_prev'))\n",
        "hitters_24 = pd.merge(hitters_24, df_22, how = 'left', on = 'Player', suffixes = (None, '_2_yrs_prev'))\n",
        "\n",
        "steamer_25 = pd.read_csv('Steamer 2025 Projections.csv')\n",
        "steamer_25 = steamer_25.dropna(axis=1, how='all')\n",
        "steamer_25['Year'] = 2024\n",
        "steamer_25['MLBAMID'] = steamer_25['MLBAMID'].fillna(-1).astype(int).astype(str)\n",
        "steamer_25['ID'] = steamer_25['MLBAMID'] + steamer_25['Year'].astype(str)\n",
        "steamer_25 = steamer_25.drop(columns = ['PlayerId', 'NameASCII', 'MLBAMID',\n",
        "                                                          'Year', 'Team', 'Name', 'R', 'CS', 'SB',\n",
        "                                                          'UBR', 'wSB', 'BsR', 'Def', 'G'])\n",
        "steamer_25 = steamer_25.rename(columns=lambda x: f\"{x}_steamer\")\n",
        "\n",
        "hitters_24 = pd.merge(hitters_24, steamer_25, how = 'left', left_on = 'ID', right_on = 'ID_steamer')\n",
        "\n",
        "ZiPS_25 = pd.read_csv('ZiPS 2025 Projections.csv')\n",
        "ZiPS_25 = ZiPS_25.dropna(axis=1, how='all')\n",
        "ZiPS_25['Year'] = 2024\n",
        "ZiPS_25['MLBAMID'] = ZiPS_25['MLBAMID'].fillna(-1).astype(str)\n",
        "ZiPS_25['ID'] = ZiPS_25['MLBAMID'] + ZiPS_25['Year'].astype(str)\n",
        "ZiPS_25 = ZiPS_25.drop(columns = ['PlayerId', 'NameASCII', 'MLBAMID',\n",
        "                                                          'Year', 'Team', 'Name', 'R', 'CS', 'SB',\n",
        "                                                           'wSB', 'BsR', 'Def', 'G'])\n",
        "\n",
        "ZiPS_25 = ZiPS_25.rename(columns=lambda x: f\"{x}_ZiPS\")\n",
        "hitters_24 = pd.merge(hitters_24, ZiPS_25, how = 'left', left_on = 'ID', right_on = 'ID_ZiPS')\n",
        "\n",
        "bp_25 = pd.read_csv('bp.csv')\n",
        "bp_25_disc = pd.read_csv('bp_disc.csv')\n",
        "bp_25_batted = pd.read_csv('bp_batted.csv')\n",
        "bp_25 = pd.merge(bp_25, bp_25_disc, how = 'inner', on = 'mlbid')\n",
        "bp_25 = pd.merge(bp_25, bp_25_batted, how = 'inner', on = 'mlbid')\n",
        "\n",
        "bp_25['ID'] = bp_25['mlbid'].astype(str) + bp_25['Season'].astype(str)\n",
        "bp_25['DRC+_upper'] = bp_25['DRC+'] + bp_25['+/-']\n",
        "bp_25['DRC+_lower'] = bp_25['DRC+'] - bp_25['+/-']\n",
        "bp_25 = bp_25.drop(columns = ['mlbid', 'Season'])\n",
        "\n",
        "hitters_24 = pd.merge(hitters_24, bp_25, how = 'left', on = 'ID')\n",
        "hitters_24.to_csv('hitters_400_PA_steamer_ZiPS_bp_25.csv')"
      ],
      "metadata": {
        "id": "1wQ-NpNmy3tA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('hitters_400_PA_upload.csv')\n",
        "df['ID'] = df['MLB_ID'].astype(str) + df['Year'].astype(str)\n",
        "\n",
        "steamer_16 = pd.read_csv('Steamer_2016_Projections.csv')\n",
        "steamer_16 = steamer_16.dropna(axis=1, how='all')\n",
        "steamer_16['Year'] = steamer_16['Year'] - 1\n",
        "steamer_16['MLBAMID'] = steamer_16['MLBAMID'].fillna(-1).astype(int).astype(str)\n",
        "steamer_16['ID'] = steamer_16['MLBAMID'] + steamer_16['Year'].astype(str)\n",
        "steamer_16 = steamer_16.drop(columns = ['PlayerId', 'NameASCII', 'MLBAMID',\n",
        "                                                          'Year', 'Team', 'Name', 'R', 'CS', 'SB',\n",
        "                                                          'UBR', 'wSB', 'BsR', 'Def', 'G'])\n",
        "\n",
        "steamer_17 = pd.read_csv('Steamer_2017_Projections.csv')\n",
        "steamer_17 = steamer_17.dropna(axis=1, how='all')\n",
        "steamer_17['Year'] = steamer_17['Year'] - 1\n",
        "steamer_17['MLBAMID'] = steamer_17['MLBAMID'].fillna(-1).astype(str)\n",
        "steamer_17['ID'] = steamer_17['MLBAMID'] + steamer_17['Year'].astype(str)\n",
        "steamer_17 = steamer_17.drop(columns = ['PlayerId', 'NameASCII', 'MLBAMID',\n",
        "                                                          'Year', 'Team', 'Name', 'R', 'CS', 'SB',\n",
        "                                                          'UBR', 'wSB', 'BsR', 'Def', 'G'])\n",
        "steamer_18 = pd.read_csv('Steamer_2018_Projections.csv')\n",
        "steamer_18 = steamer_18.dropna(axis=1, how='all')\n",
        "steamer_18['Year'] = steamer_18['Year'] - 1\n",
        "steamer_18['MLBAMID'] = steamer_18['MLBAMID'].fillna(-1).astype(str)\n",
        "steamer_18['ID'] = steamer_18['MLBAMID'] + steamer_18['Year'].astype(str)\n",
        "steamer_18 = steamer_18.drop(columns = ['PlayerId', 'NameASCII', 'MLBAMID',\n",
        "                                                          'Year', 'Team', 'Name', 'R', 'CS', 'SB',\n",
        "                                                          'UBR', 'wSB', 'BsR', 'Def', 'G'])\n",
        "steamer_19 = pd.read_csv('Steamer_2019_Projections.csv')\n",
        "steamer_19 = steamer_19.dropna(axis=1, how='all')\n",
        "steamer_19['Year'] = steamer_19['Year'] - 1\n",
        "steamer_19['MLBAMID'] = steamer_19['MLBAMID'].fillna(-1).astype(str)\n",
        "steamer_19['ID'] = steamer_19['MLBAMID'] + steamer_19['Year'].astype(str)\n",
        "steamer_19 = steamer_19.drop(columns = ['PlayerId', 'NameASCII', 'MLBAMID',\n",
        "                                                          'Year', 'Team', 'Name', 'R', 'CS', 'SB',\n",
        "                                                          'UBR', 'wSB', 'BsR', 'Def', 'G'])\n",
        "steamer_22 = pd.read_csv('Steamer_2022_Projections.csv')\n",
        "steamer_22 = steamer_22.dropna(axis=1, how='all')\n",
        "steamer_22['Year'] = steamer_22['Year'] - 1\n",
        "steamer_22['MLBAMID'] = steamer_22['MLBAMID'].fillna(-1).astype(int).astype(str)\n",
        "steamer_22['ID'] = steamer_22['MLBAMID'] + steamer_22['Year'].astype(str)\n",
        "steamer_22 = steamer_22.drop(columns = ['PlayerId', 'NameASCII', 'MLBAMID',\n",
        "                                                          'Year', 'Team', 'Name', 'R', 'CS', 'SB',\n",
        "                                                          'UBR', 'wSB', 'BsR', 'Def', 'G'])\n",
        "steamer_23 = pd.read_csv('Steamer_2023_Projections.csv')\n",
        "steamer_23 = steamer_23.dropna(axis=1, how='all')\n",
        "steamer_23['Year'] = steamer_23['Year'] - 1\n",
        "steamer_23['MLBAMID'] = steamer_23['MLBAMID'].fillna(-1).astype(int).astype(str)\n",
        "steamer_23['ID'] = steamer_23['MLBAMID'] + steamer_23['Year'].astype(str)\n",
        "steamer_23 = steamer_23.drop(columns = ['PlayerId', 'NameASCII', 'MLBAMID',\n",
        "                                                          'Year', 'Team', 'Name', 'R', 'CS', 'SB',\n",
        "                                                          'UBR', 'wSB', 'BsR', 'Def', 'G'])\n",
        "steamer_24 = pd.read_csv('Steamer_2024_Projections.csv')\n",
        "steamer_24 = steamer_24.dropna(axis=1, how='all')\n",
        "steamer_24['Year'] = steamer_24['Year'] - 1\n",
        "steamer_24['MLBAMID'] = steamer_24['MLBAMID'].fillna(-1).astype(int).astype(str)\n",
        "steamer_24['ID'] = steamer_24['MLBAMID'] + steamer_24['Year'].astype(str)\n",
        "steamer_24 = steamer_24.drop(columns = ['PlayerId', 'NameASCII', 'MLBAMID',\n",
        "                                                          'Year', 'Team', 'Name', 'R', 'CS', 'SB',\n",
        "                                                          'UBR', 'wSB', 'BsR', 'Def', 'G'])\n",
        "\n",
        "steamer_25 = pd.read_csv('Steamer_2025_Projections.csv')\n",
        "steamer_25 = steamer_25.dropna(axis=1, how='all')\n",
        "steamer_25['Year'] = 2024\n",
        "steamer_25['MLBAMID'] = steamer_25['MLBAMID'].fillna(-1).astype(int).astype(str)\n",
        "steamer_25['ID'] = steamer_25['MLBAMID'] + steamer_25['Year'].astype(str)\n",
        "steamer_25 = steamer_25.drop(columns = ['PlayerId', 'NameASCII', 'MLBAMID',\n",
        "                                                          'Year', 'Team', 'Name', 'R', 'CS', 'SB',\n",
        "                                                          'UBR', 'wSB', 'BsR', 'Def', 'G'])\n",
        "\n",
        "steamer_projections = pd.concat([steamer_16, steamer_17, steamer_18, steamer_19, steamer_22, steamer_23, steamer_24])\n",
        "steamer_projections = steamer_projections.rename(columns=lambda x: f\"{x}_steamer\")\n",
        "df = pd.merge(df, steamer_projections, how = 'left', left_on = 'ID', right_on = 'ID_steamer')\n",
        "df.to_csv('hitters_400_PA_steamer.csv')"
      ],
      "metadata": {
        "id": "Qd3aZgxFq3cR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('hitters_400_PA_steamer.csv')\n",
        "df['ID'] = df['MLB_ID'].astype(str) + df['Year'].astype(str)\n",
        "\n",
        "ZiPS_16 = pd.read_csv('ZiPS_2016_Projections.csv')\n",
        "ZiPS_16 = ZiPS_16.dropna(axis=1, how='all')\n",
        "ZiPS_16['Year'] = 2015\n",
        "ZiPS_16['MLBAMID'] = ZiPS_16['MLBAMID'].fillna(-1).astype(int).astype(str)\n",
        "ZiPS_16['ID'] = ZiPS_16['MLBAMID'] + ZiPS_16['Year'].astype(str)\n",
        "ZiPS_16 = ZiPS_16.drop(columns = ['PlayerId', 'NameASCII', 'MLBAMID',\n",
        "                                                          'Year', 'Team', 'Name', 'R', 'CS', 'SB',\n",
        "                                                          'wSB', 'BsR', 'Def', 'G'])\n",
        "ZiPS_17 = pd.read_csv('ZiPS_2017_Projections.csv')\n",
        "ZiPS_17 = ZiPS_17.dropna(axis=1, how='all')\n",
        "ZiPS_17['Year'] = 2016\n",
        "ZiPS_17['MLBAMID'] = ZiPS_17['MLBAMID'].fillna(-1).astype(int).astype(str)\n",
        "ZiPS_17['ID'] = ZiPS_17['MLBAMID'] + ZiPS_17['Year'].astype(str)\n",
        "ZiPS_17 = ZiPS_17.drop(columns = ['PlayerId', 'NameASCII', 'MLBAMID',\n",
        "                                                          'Year', 'Team', 'Name', 'R', 'CS', 'SB',\n",
        "                                                          'wSB', 'BsR', 'Def', 'G'])\n",
        "ZiPS_18 = pd.read_csv('ZiPS_2018_Projections.csv')\n",
        "ZiPS_18 = ZiPS_18.dropna(axis=1, how='all')\n",
        "ZiPS_18['Year'] = 2017\n",
        "ZiPS_18['MLBAMID'] = ZiPS_18['MLBAMID'].fillna(-1).astype(str)\n",
        "ZiPS_18['ID'] = ZiPS_18['MLBAMID'] + ZiPS_18['Year'].astype(str)\n",
        "ZiPS_18 = ZiPS_18.drop(columns = ['PlayerId', 'NameASCII', 'MLBAMID',\n",
        "                                                          'Year', 'Team', 'Name', 'R', 'CS', 'SB',\n",
        "                                                          'wSB', 'BsR', 'Def', 'G'])\n",
        "ZiPS_19 = pd.read_csv('ZiPS_2019_Projections.csv')\n",
        "ZiPS_19 = ZiPS_19.dropna(axis=1, how='all')\n",
        "ZiPS_19['Year'] = 2018\n",
        "ZiPS_19['MLBAMID'] = ZiPS_19['MLBAMID'].fillna(-1).astype(str)\n",
        "ZiPS_19['ID'] = ZiPS_19['MLBAMID'] + ZiPS_19['Year'].astype(str)\n",
        "ZiPS_19 = ZiPS_19.drop(columns = ['PlayerId', 'NameASCII', 'MLBAMID',\n",
        "                                                          'Year', 'Team', 'Name', 'R', 'CS', 'SB',\n",
        "                                                           'wSB', 'BsR', 'Def', 'G'])\n",
        "ZiPS_22 = pd.read_csv('ZiPS_2022_Projections.csv')\n",
        "ZiPS_22 = ZiPS_22.dropna(axis=1, how='all')\n",
        "ZiPS_22['Year'] = 2021\n",
        "ZiPS_22['MLBAMID'] = ZiPS_22['MLBAMID'].fillna(-1).astype(str)\n",
        "ZiPS_22['ID'] = ZiPS_22['MLBAMID'] + ZiPS_22['Year'].astype(str)\n",
        "ZiPS_22 = ZiPS_22.drop(columns = ['PlayerId', 'NameASCII', 'MLBAMID',\n",
        "                                                          'Year', 'Team', 'Name', 'R', 'CS', 'SB',\n",
        "                                                           'wSB', 'BsR', 'Def', 'G'])\n",
        "ZiPS_23 = pd.read_csv('ZiPS_2023_Projections.csv')\n",
        "ZiPS_23 = ZiPS_23.dropna(axis=1, how='all')\n",
        "ZiPS_23['Year'] = 2022\n",
        "ZiPS_23['MLBAMID'] = ZiPS_23['MLBAMID'].fillna(-1).astype(str)\n",
        "ZiPS_23['ID'] = ZiPS_23['MLBAMID'] + ZiPS_23['Year'].astype(str)\n",
        "ZiPS_23 = ZiPS_23.drop(columns = ['PlayerId', 'NameASCII', 'MLBAMID',\n",
        "                                                          'Year', 'Team', 'Name', 'R', 'CS', 'SB',\n",
        "                                                           'wSB', 'BsR', 'Def', 'G'])\n",
        "ZiPS_24 = pd.read_csv('ZiPS_2024_Projections.csv')\n",
        "ZiPS_24 = ZiPS_24.dropna(axis=1, how='all')\n",
        "ZiPS_24['Year'] = 2023\n",
        "ZiPS_24['MLBAMID'] = ZiPS_24['MLBAMID'].fillna(-1).astype(str)\n",
        "ZiPS_24['ID'] = ZiPS_24['MLBAMID'] + ZiPS_24['Year'].astype(str)\n",
        "ZiPS_24 = ZiPS_24.drop(columns = ['PlayerId', 'NameASCII', 'MLBAMID',\n",
        "                                                          'Year', 'Team', 'Name', 'R', 'CS', 'SB',\n",
        "                                                           'wSB', 'BsR', 'Def', 'G'])\n",
        "\n",
        "ZiPS_25 = pd.read_csv('ZiPS_2025_Projections.csv')\n",
        "ZiPS_25 = ZiPS_25.dropna(axis=1, how='all')\n",
        "ZiPS_25['Year'] = 2024\n",
        "ZiPS_25['MLBAMID'] = ZiPS_25['MLBAMID'].fillna(-1).astype(str)\n",
        "ZiPS_25['ID'] = ZiPS_25['MLBAMID'] + ZiPS_25['Year'].astype(str)\n",
        "ZiPS_25 = ZiPS_25.drop(columns = ['PlayerId', 'NameASCII', 'MLBAMID',\n",
        "                                                          'Year', 'Team', 'Name', 'R', 'CS', 'SB',\n",
        "                                                           'wSB', 'BsR', 'Def', 'G'])\n",
        "\n",
        "ZiPS_projections = pd.concat([ZiPS_16, ZiPS_17, ZiPS_18, ZiPS_19, ZiPS_22, ZiPS_23, ZiPS_24])\n",
        "ZiPS_projections = ZiPS_projections.rename(columns=lambda x: f\"{x}_ZiPS\")\n",
        "df = pd.merge(df, ZiPS_projections, how = 'left', left_on = 'ID', right_on = 'ID_ZiPS')\n",
        "df.to_csv('hitters_400_PA_steamer_ZiPS.csv')"
      ],
      "metadata": {
        "id": "7lpJCcUWr0Jv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bp_15 = pd.read_csv('bp_2015.csv')\n",
        "bp_15_disc = pd.read_csv('bp_2015_disc.csv')\n",
        "bp_15_batted = pd.read_csv('bp_2015_batted.csv')\n",
        "bp_15 = pd.merge(bp_15, bp_15_disc, how = 'inner', on = 'mlbid')\n",
        "bp_15 = pd.merge(bp_15, bp_15_batted, how = 'inner', on = 'mlbid')\n",
        "\n",
        "bp_16 = pd.read_csv('bp_2016.csv')\n",
        "bp_16_disc = pd.read_csv('bp_2016_disc.csv')\n",
        "bp_16_batted = pd.read_csv('bp_2016_batted.csv')\n",
        "bp_16 = pd.merge(bp_16, bp_16_disc, how = 'inner', on = 'mlbid')\n",
        "bp_16 = pd.merge(bp_16, bp_16_batted, how = 'inner', on = 'mlbid')\n",
        "\n",
        "bp_17 = pd.read_csv('bp_2017.csv')\n",
        "bp_17_disc = pd.read_csv('bp_2017_disc.csv')\n",
        "bp_17_batted = pd.read_csv('bp_2017_batted.csv')\n",
        "bp_17 = pd.merge(bp_17, bp_17_disc, how = 'inner', on = 'mlbid')\n",
        "bp_17 = pd.merge(bp_17, bp_17_batted, how = 'inner', on = 'mlbid')\n",
        "\n",
        "bp_18 = pd.read_csv('bp_2018.csv')\n",
        "bp_18_disc = pd.read_csv('bp_2018_disc.csv')\n",
        "bp_18_batted = pd.read_csv('bp_2018_batted.csv')\n",
        "bp_18 = pd.merge(bp_18, bp_18_disc, how = 'inner', on = 'mlbid')\n",
        "bp_18 = pd.merge(bp_18, bp_18_batted, how = 'inner', on = 'mlbid')\n",
        "\n",
        "bp_21 = pd.read_csv('bp_2021.csv')\n",
        "bp_21_disc = pd.read_csv('bp_2021_disc.csv')\n",
        "bp_21_batted = pd.read_csv('bp_2021_batted.csv')\n",
        "bp_21 = pd.merge(bp_21, bp_21_disc, how = 'inner', on = 'mlbid')\n",
        "bp_21 = pd.merge(bp_21, bp_21_batted, how = 'inner', on = 'mlbid')\n",
        "\n",
        "bp_22 = pd.read_csv('bp_2022.csv')\n",
        "bp_22_disc = pd.read_csv('bp_2022_disc.csv')\n",
        "bp_22_batted = pd.read_csv('bp_2022_batted.csv')\n",
        "bp_22 = pd.merge(bp_22, bp_22_disc, how = 'inner', on = 'mlbid')\n",
        "bp_22 = pd.merge(bp_22, bp_22_batted, how = 'inner', on = 'mlbid')\n",
        "\n",
        "bp_23 = pd.read_csv('bp_2023.csv')\n",
        "bp_23_disc = pd.read_csv('bp_2023_disc.csv')\n",
        "bp_23_batted = pd.read_csv('bp_2023_batted.csv')\n",
        "bp_23 = pd.merge(bp_23, bp_23_disc, how = 'inner', on = 'mlbid')\n",
        "bp_23 = pd.merge(bp_23, bp_23_batted, how = 'inner', on = 'mlbid')\n",
        "\n",
        "bp_24 = pd.read_csv('bp_2023.csv')\n",
        "bp_24_disc = pd.read_csv('bp_2023_disc.csv')\n",
        "bp_24_batted = pd.read_csv('bp_2023_batted.csv')\n",
        "bp_24 = pd.merge(bp_24, bp_24_disc, how = 'inner', on = 'mlbid')\n",
        "bp_24 = pd.merge(bp_24, bp_24_batted, how = 'inner', on = 'mlbid')\n",
        "\n",
        "bp_25 = pd.read_csv('bp_2025.csv')\n",
        "bp_25_disc = pd.read_csv('bp_2025_disc.csv')\n",
        "bp_25_batted = pd.read_csv('bp_2025_batted.csv')\n",
        "bp_25 = pd.merge(bp_25, bp_25_disc, how = 'inner', on = 'mlbid')\n",
        "bp_25 = pd.merge(bp_25, bp_25_batted, how = 'inner', on = 'mlbid')\n",
        "\n",
        "bp = pd.concat([bp_15, bp_16, bp_17, bp_18, bp_21, bp_22, bp_23, bp_24, bp_25])\n",
        "bp = bp.drop(columns = ['bpid_x', 'Name_x', 'Age_x', 'Season_x', 'Team_x', 'bpid_y',\n",
        "                              'Name_y', 'Age_y', 'Season_y', 'Team_y', 'Pitches', 'bpid',\n",
        "                              'Name', 'Team'])\n",
        "bp['ID'] = bp['mlbid'].astype(str) + bp['Season'].astype(str)\n",
        "bp['DRC+_upper'] = bp['DRC+'] + bp['+/-']\n",
        "bp['DRC+_lower'] = bp['DRC+'] - bp['+/-']\n",
        "bp = bp.drop(columns = ['mlbid', 'Season'])\n",
        "bp.to_csv('bp_400_PA.csv')"
      ],
      "metadata": {
        "collapsed": true,
        "id": "2CK9kC5d7kDM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('hitters_400_PA_steamer_ZiPS.csv')\n",
        "bp['ID'] = bp['ID'].astype(int)\n",
        "df = pd.merge(df, bp, how = 'left', on = 'ID')\n",
        "df = df.drop(columns = ['Unnamed: 0.1', 'Unnamed: 0'])\n",
        "df.to_csv('hitters_400_PA_steamer_ZiPS_bp.csv')"
      ],
      "metadata": {
        "collapsed": true,
        "id": "1VcWerR7eFxh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Predict wOBA**"
      ],
      "metadata": {
        "id": "_E3ua48mOCTy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ---- Imports ----\n",
        "!pip install optuna lightgbm\n",
        "import os, optuna\n",
        "import lightgbm as lgb\n",
        "from sklearn.model_selection import TimeSeriesSplit\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler, PolynomialFeatures\n",
        "from sklearn.linear_model import ElasticNet, BayesianRidge, Ridge, Lasso\n",
        "from sklearn.ensemble import StackingRegressor\n",
        "from scipy.optimize import minimize\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "sns.set()\n",
        "\n",
        "# ---- Load ----\n",
        "df = pd.read_csv('hitters_400_PA_steamer_ZiPS_bp.csv')\n",
        "df[\"Year\"] = df[\"Year\"].astype(int)\n",
        "df = df[df['Year'] != 2024]\n",
        "df = df.drop_duplicates().reset_index(drop=True)\n",
        "df = df.fillna(0)\n",
        "# df = df[df['PA_prev']>=100]"
      ],
      "metadata": {
        "id": "3T3akzbSqB2X",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pd.options.mode.chained_assignment = None\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
        "warnings.filterwarnings(\"ignore\", category=ImportWarning)"
      ],
      "metadata": {
        "id": "kMLGeaiKivYb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import scipy.stats\n",
        "def correlation(dataset, threshold, target): #Function to get Pearson's correlation between input and target\n",
        "  data = []\n",
        "  cols = []\n",
        "  correlations = []\n",
        "  #corS = 0\n",
        "  if isinstance(target, np.ndarray):\n",
        "    target = pd.Series(target)\n",
        "  for col in dataset.columns:\n",
        "      #print(dataset.loc[:,col])\n",
        "      #print(col)\n",
        "      corS = dataset.loc[:,col].corr(target, method='spearman') # 'kendall'\n",
        "      corP = dataset.loc[:,col].corr(target)\n",
        "      if (abs(corP) > threshold) or (abs(corS) > threshold):\n",
        "        cor2 = max(abs(corP), abs(corS))\n",
        "        data.append(dataset.loc[:,col]) #make list of columns that meet the threshold\n",
        "        cols.append(col)\n",
        "        correlations.append(cor2) #make list of correlations that meet the threshold\n",
        "  if len(data) == 0:\n",
        "     return pd.DataFrame()\n",
        "\n",
        "  df = pd.DataFrame(data)\n",
        "  df_len = len(df.columns)\n",
        "  df.insert(df_len, 'corrs', correlations)\n",
        "  df = df.sort_values(by=df.columns[-1], ascending=False, key = abs)\n",
        "  df = df.transpose()\n",
        "  df_corrs = df.iloc[-1:, :]\n",
        "  df = df.drop(df.tail(1).index)\n",
        "  return df#, df_corrs"
      ],
      "metadata": {
        "collapsed": true,
        "id": "CUQow8Glk2ue"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import make_swiss_roll\n",
        "from sklearn.manifold import LocallyLinearEmbedding\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.cross_decomposition import PLSRegression\n",
        "def reduce_df(x_tr, x_te, y_tr, reduction_choice):\n",
        "  if reduction_choice == \"PLS\":\n",
        "    pls = PLSRegression(n_components=34)\n",
        "    X_tr_pls = pls.fit_transform(x_tr, y_tr)[0]\n",
        "    X_te_pls = pls.transform(x_te)\n",
        "    print(\"Explained variance in X:\", np.round(pls.x_scores_.var(axis=0) / x_tr.var(axis=0).sum(), 3))\n",
        "    print(\"Explained variance in Y:\", np.round(pls.y_scores_.var(axis=0) / y_tr.var(), 3))\n",
        "    return X_tr_pls, X_te_pls, pls\n",
        "\n",
        "  if reduction_choice == \"PCA\":\n",
        "    pca=PCA(n_components = None, random_state=28) #n_components = None, 17, 34\n",
        "    X_tr_PCA = pca.fit_transform(x_tr)\n",
        "    X_te_PCA = pca.transform(x_te)\n",
        "    print(\"Principal axes:\\n\", pca.components_.tolist())\n",
        "    print(\"Explained variance:\\n\", pca.explained_variance_.tolist())\n",
        "    print(\"Mean:\", pca.mean_)\n",
        "    return X_tr_PCA, X_te_PCA, pca"
      ],
      "metadata": {
        "id": "ETfASjaUyBXX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define features and target\n",
        "cols_to_drop = ['ID_ZiPS', 'ID_steamer', \"Year\", \"wOBA_next\",\n",
        "                'MLB_ID', 'ID']\n",
        "\n",
        "# df = df[df[\"Year\"] != 2015]\n",
        "X = df.drop(columns = cols_to_drop)\n",
        "y = df[\"wOBA_next\"]\n",
        "\n",
        "# Clean up data: replace commas and convert to numeric\n",
        "for col in X.columns:\n",
        "    if X[col].dtype == 'object':\n",
        "        try:\n",
        "            X[col] = X[col].astype(str).str.replace(',', '', regex=False)\n",
        "            X[col] = pd.to_numeric(X[col])\n",
        "        except ValueError:\n",
        "            pass # Keep column as object if conversion fails after removing commas\n",
        "\n",
        "# Hold out the most recent season for final testing\n",
        "max_season = 2023\n",
        "X_train = X[df[\"Year\"] < max_season]\n",
        "y_train = y[df[\"Year\"] < max_season]\n",
        "X_holdout = X[df[\"Year\"] == max_season]\n",
        "y_holdout = y[df[\"Year\"] == max_season]\n",
        "\n",
        "# -----------------\n",
        "# Walk-forward CV\n",
        "# -----------------\n",
        "def walk_forward_cv(model, X, y, n_splits=5):\n",
        "    tscv = TimeSeriesSplit(n_splits=n_splits)\n",
        "    rmses = []\n",
        "    for train_idx, val_idx in tscv.split(X):\n",
        "        X_tr, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
        "        y_tr, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
        "\n",
        "        X_tr_top = correlation(X_tr, 0.1, y_tr)\n",
        "        top_features = X_tr_top.columns[:5]\n",
        "        X_tr_top = X_tr[top_features]\n",
        "        X_val_top = X_val[top_features]\n",
        "        interaction_transformer = PolynomialFeatures(degree=2, interaction_only=True, include_bias=False)\n",
        "        X_tr_top = pd.DataFrame(interaction_transformer.fit_transform(X_tr_top), columns = interaction_transformer.get_feature_names_out())\n",
        "        X_val_top = pd.DataFrame(interaction_transformer.transform(X_val_top), columns = interaction_transformer.get_feature_names_out())\n",
        "        X_val = X_val.reset_index(drop = True)\n",
        "        X_tr = X_tr.reset_index(drop = True)\n",
        "        X_tr = pd.concat([X_tr, X_tr_top], axis=1)\n",
        "        X_val = pd.concat([X_val, X_val_top], axis=1)\n",
        "\n",
        "        scaler = MinMaxScaler() #MinMaxScaler StandardScaler\n",
        "\n",
        "        X_tr = pd.DataFrame(scaler.fit_transform(X_tr), columns = X_tr.columns)\n",
        "        X_val = pd.DataFrame(scaler.transform(X_val), columns = X_val.columns)\n",
        "        # y_tr = scaler.fit_transform(y_tr.values.reshape(-1, 1)).ravel()\n",
        "        # X_tr, X_val, PLS = reduce_df(X_tr, X_val, y_tr, \"PLS\")\n",
        "\n",
        "        model.fit(X_tr, y_tr)\n",
        "        preds = model.predict(X_val)\n",
        "        # preds = scaler.inverse_transform(preds.reshape(-1, 1)).ravel()\n",
        "        rmse = np.sqrt(mean_squared_error(y_val, preds))\n",
        "        # mae = mean_absolute_error(y_val, preds)\n",
        "        rmses.append(rmse)\n",
        "    return np.mean(rmses)#, np.mean(maes)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "t_O64VV2pky5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Optuna objectives:\n",
        "def objective_Ridge(trial):\n",
        "    alpha = trial.suggest_float(\"alpha\", 1e-4, 10, log=True)#1e-4, 10.0; Alpha is the regularization strength\n",
        "\n",
        "    model = Ridge(alpha=alpha, random_state=28)\n",
        "    mean_rmse = walk_forward_cv(model, X_train, y_train, n_splits=5)\n",
        "    return mean_rmse\n",
        "\n",
        "def objective_elastic(trial):\n",
        "    alpha = trial.suggest_float(\"alpha\", 1e-4, 10, log = True) #1e-4, 10.0\n",
        "    l1_ratio = trial.suggest_float(\"l1_ratio\", 0.0, 1.0) #0, 1\n",
        "    model = ElasticNet(alpha=alpha, l1_ratio=l1_ratio, max_iter=100000)\n",
        "    mean_rmse = walk_forward_cv(model, X_train, y_train, n_splits=5)\n",
        "    return mean_rmse\n",
        "\n",
        "def objective_bayes(trial):\n",
        "    alpha_1 = trial.suggest_float(\"alpha_1\", 1e-6, 1e-1, log = True) #1e-6, 1e-1\n",
        "    alpha_2 = trial.suggest_float(\"alpha_2\", 1e-6, 1e-1, log = True)\n",
        "    lambda_1 = trial.suggest_float(\"lambda_1\", 1e-6, 1e-1, log = True)\n",
        "    lambda_2 = trial.suggest_float(\"lambda_2\", 1e-6, 1e-1, log = True)\n",
        "\n",
        "    model = BayesianRidge(\n",
        "        alpha_1=alpha_1,\n",
        "        alpha_2=alpha_2,\n",
        "        lambda_1=lambda_1,\n",
        "        lambda_2=lambda_2,\n",
        "    )\n",
        "    mean_rmse = walk_forward_cv(model, X_train, y_train, n_splits=5)\n",
        "    #print(f\"BayesianRidge trial {trial.number} -> avg RMSE: {mean_rmse:.4f}\")\n",
        "    return mean_rmse"
      ],
      "metadata": {
        "id": "jwb66xVEIacf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Tuning ElasticNet...\")\n",
        "study_en = optuna.create_study(direction=\"minimize\")\n",
        "study_en.optimize(objective_elastic, n_trials=150)\n",
        "\n",
        "print(\"\\nBest ElasticNet params:\", study_en.best_params)\n",
        "print(\"Best ElasticNet CV RMSE:\", round(study_en.best_value, 4))\n",
        "\n",
        "print(\"Tuning Ridge...\")\n",
        "study_Ridge = optuna.create_study(direction=\"minimize\")\n",
        "study_Ridge.optimize(objective_Ridge, n_trials=150)\n",
        "\n",
        "print(\"\\nBest Ridge params:\", study_Ridge.best_params)\n",
        "print(\"Best Ridge CV RMSE:\", round(study_Ridge.best_value, 4))\n",
        "\n",
        "print(\"\\nTuning Bayesian Ridge...\")\n",
        "study_bayes = optuna.create_study(direction=\"minimize\")\n",
        "study_bayes.optimize(objective_bayes, n_trials=150)\n",
        "\n",
        "print(\"\\nBest Bayesian Ridge params:\", study_bayes.best_params)\n",
        "print(\"Best Bayesian Ridge CV RMSE:\", round(study_bayes.best_value, 4))"
      ],
      "metadata": {
        "collapsed": true,
        "id": "CQVQ_VNl9lHh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*   no 2015 - .0274, MMS, top 5 poly, Elastic: {'alpha': 0.01690412309073022, 'l1_ratio': 0.00488520276713722}"
      ],
      "metadata": {
        "id": "-KmoFMGKQ9UN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Holdout**"
      ],
      "metadata": {
        "id": "mshoGJYDwIi6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def scores(y_true, preds):\n",
        "  rmse = np.sqrt(mean_squared_error(y_true, preds))\n",
        "  mae = mean_absolute_error(y_true, preds)\n",
        "  bias = np.mean(preds - y_true)\n",
        "  return round(float(rmse), 4), round(mae, 4), round(float(bias), 4)"
      ],
      "metadata": {
        "id": "-z7H8F2VTcH7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Preds:\n",
        "max_season = 2023\n",
        "X_train = X[df[\"Year\"] < max_season]\n",
        "y_train = y[df[\"Year\"] < max_season]\n",
        "X_holdout = X[df[\"Year\"] == max_season]\n",
        "y_holdout = y[df[\"Year\"] == max_season]\n",
        "\n",
        "best_ridge = Ridge(alpha = 9.952496159351869)\n",
        "best_bayes = BayesianRidge(alpha_1= 0.0006563106706162612, alpha_2= 0.09755014992040287, lambda_1= 0.0009410610394146774, lambda_2= 0.0008934520454619358)\n",
        "\n",
        "steam_preds = X_holdout['wOBA_steamer']\n",
        "ZiPS_preds = X_holdout['wOBA_ZiPS']\n",
        "\n",
        "#Scale:\n",
        "scaler = MinMaxScaler() #MinMaxScaler StandardScaler\n",
        "X_train = pd.DataFrame(scaler.fit_transform(X_train), columns = X_train.columns)\n",
        "X_holdout = pd.DataFrame(scaler.transform(X_holdout), columns = X_holdout.columns)\n",
        "\n",
        "# Fit models\n",
        "best_ridge.fit(X_train, y_train)\n",
        "best_bayes.fit(X_train, y_train)\n",
        "\n",
        "# Predictions\n",
        "preds_ridge = best_ridge.predict(X_holdout)\n",
        "preds_bayes = best_bayes.predict(X_holdout)\n",
        "\n",
        "#> 2015 Models\n",
        "df2 = df[df['Year'] > 2015]\n",
        "X = df2.drop(columns = cols_to_drop)\n",
        "y = df2[\"wOBA_next\"]\n",
        "\n",
        "for col in X.columns:\n",
        "    if X[col].dtype == 'object':\n",
        "        try:\n",
        "            X[col] = X[col].astype(str).str.replace(',', '', regex=False)\n",
        "            X[col] = pd.to_numeric(X[col])\n",
        "        except ValueError:\n",
        "            pass # Keep column as object if conversion fails after removing commas\n",
        "\n",
        "X_train = X[df2[\"Year\"] < max_season]\n",
        "y_train = y[df2[\"Year\"] < max_season]\n",
        "X_holdout2 = X[df2[\"Year\"] == max_season]\n",
        "\n",
        "best_en = ElasticNet(alpha = 0.01690412309073022, l1_ratio = 0.00488520276713722)\n",
        "best_bayes = BayesianRidge(alpha_1 = 1.4486342617422766e-05, alpha_2 = 0.0970675845833932, lambda_1 = 0.03708029637023936, lambda_2 = 0.00032691530125994326)\n",
        "\n",
        "#Get top 5 features:\n",
        "X_tr_top = correlation(X_train, 0.1, y_train)\n",
        "top_features = X_tr_top.columns[:5]\n",
        "X_tr_top = X_train[top_features]\n",
        "X_holdout2_top = X_holdout2[top_features]\n",
        "interaction_transformer = PolynomialFeatures(degree=2, interaction_only=True, include_bias=False)\n",
        "X_tr_top = pd.DataFrame(interaction_transformer.fit_transform(X_tr_top), columns = interaction_transformer.get_feature_names_out())\n",
        "X_holdout2_top = pd.DataFrame(interaction_transformer.transform(X_holdout2_top), columns = interaction_transformer.get_feature_names_out())\n",
        "\n",
        "X_holdout2 = X_holdout2.reset_index(drop = True)\n",
        "X_train = X_train.reset_index(drop = True)\n",
        "X_train = pd.concat([X_train, X_tr_top], axis=1)\n",
        "X_holdout2 = pd.concat([X_holdout2, X_holdout2_top], axis=1)\n",
        "\n",
        "#Scale:\n",
        "scaler2 = MinMaxScaler() #MinMaxScaler StandardScaler\n",
        "wOBA_ZiPS_train = X_train['wOBA_ZiPS']\n",
        "wOBA_ZiPS_train = wOBA_ZiPS_train.iloc[:, 0]\n",
        "wOBA_Steam_train = X_train['wOBA_steamer']\n",
        "wOBA_Steam_train = wOBA_Steam_train.iloc[:, 0]\n",
        "X_train = pd.DataFrame(scaler2.fit_transform(X_train), columns = X_train.columns)\n",
        "X_holdout2 = pd.DataFrame(scaler2.transform(X_holdout2), columns = X_holdout2.columns)\n",
        "\n",
        "# Fit models\n",
        "best_en.fit(X_train, y_train)\n",
        "best_bayes.fit(X_train, y_train)\n",
        "\n",
        "# Predictions\n",
        "preds_en = best_en.predict(X_holdout2)\n",
        "preds_bayes2 = best_bayes.predict(X_holdout2)"
      ],
      "metadata": {
        "id": "kh2CPNMCKmze"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wOBA_train_preds_en = best_en.predict(X_train)\n",
        "wOBA_train_preds = (wOBA_ZiPS_train + wOBA_train_preds_en)/2\n",
        "ensemble_train_scores = scores(y_train, wOBA_train_preds)\n",
        "ZiPS_train_scores = scores(y_train, wOBA_ZiPS_train)\n",
        "Steamer_train_scores = scores(y_train, wOBA_Steam_train)\n",
        "print(\"\\nTraining Results (RMSE, MAE, Bias):\")\n",
        "print(\"ZiPS and ElasticNet:\", ensemble_train_scores)\n",
        "print(\"ZiPS:\", ZiPS_train_scores)\n",
        "print(\"Steamer:\", Steamer_train_scores)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EtW4otyHv77i",
        "outputId": "7b318e28-772d-4d3f-9ec2-53a1501e9e68"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training Results (RMSE, MAE, Bias):\n",
            "ZiPS and ElasticNet: (0.0273, 0.0215, -0.0004)\n",
            "ZiPS: (0.0294, 0.0231, 0.0)\n",
            "Steamer: (0.0291, 0.0232, 0.0011)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Holdout Evaluation/Ensembles**"
      ],
      "metadata": {
        "id": "TbrBwmhWc6Uk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Bootstrap test for RMSE and MAE difference\n",
        "def bootrap_scores(ensemble_preds, public_preds, holdout_preds, choice):\n",
        "  rng = np.random.default_rng(123)\n",
        "  B = 10000\n",
        "  n = len(holdout_preds)\n",
        "  diffs = np.empty(B)\n",
        "  diffs_mae = np.empty(B)\n",
        "  holdout_preds = holdout_preds.reset_index(drop = True)\n",
        "  public_preds = public_preds.reset_index(drop = True)\n",
        "  ensemble_preds = ensemble_preds.reset_index(drop = True)\n",
        "\n",
        "  for i in range(B):\n",
        "      idx = rng.integers(0, n, n)\n",
        "      public_preds_rmse = np.sqrt(mean_squared_error(holdout_preds[idx], public_preds[idx])) #worse RMSE model\n",
        "      public_preds_mae = mean_absolute_error(holdout_preds[idx], public_preds[idx])\n",
        "      ensemble_rmse = np.sqrt(mean_squared_error(holdout_preds[idx], ensemble_preds[idx]))\n",
        "      ensemble_mae = mean_absolute_error(holdout_preds[idx], ensemble_preds[idx])\n",
        "      diffs[i] = public_preds_rmse - ensemble_rmse\n",
        "      diffs_mae[i] = public_preds_mae - ensemble_mae\n",
        "\n",
        "  p_val = np.mean(diffs <= 0)  # probability public preds are NOT worse (<=0 means model 1 is better or equal)\n",
        "  p_val_mae = np.mean(diffs_mae <= 0)  # probability public preds are NOT worse (<=0 means model 1 is better or equal)\n",
        "  print(\"Bootstrap p-value (prob\", choice, \"rmse <= Ensemble rmse):\", round(p_val,3))\n",
        "  print(\"Bootstrap p-value (prob\", choice, \"mae <= Ensemble mae):\", round(p_val_mae,3))"
      ],
      "metadata": {
        "id": "5DNWukf0_nh0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Ensembles:\n",
        "preds_ridge_bayes_ZiPS_steam = (preds_ridge + preds_bayes + ZiPS_preds + steam_preds)/4\n",
        "preds_all = (preds_ridge + preds_bayes + ZiPS_preds + steam_preds + preds_en + preds_bayes2)/6\n",
        "\n",
        "preds_ZiPS_ridge = (preds_ridge + ZiPS_preds)/2\n",
        "preds_ZiPS_bayes = (preds_bayes + ZiPS_preds)/2\n",
        "preds_ZiPS_en = (preds_en + ZiPS_preds)/2\n",
        "\n",
        "#Scores:\n",
        "ridge_scores = scores(y_holdout, preds_ridge)\n",
        "bayes_scores = scores(y_holdout, preds_bayes)\n",
        "bayes2_scores = scores(y_holdout, preds_bayes2)\n",
        "elastic_scores = scores(y_holdout, preds_en)\n",
        "ZiPS_scores = scores(y_holdout, ZiPS_preds)\n",
        "steam_scores = scores(y_holdout, steam_preds)\n",
        "\n",
        "ZiPS_en_scores = scores(y_holdout, preds_ZiPS_en)\n",
        "ZiPS_ridge_scores = scores(y_holdout, preds_ZiPS_ridge)\n",
        "ZiPS_bayes_scores = scores(y_holdout, preds_ZiPS_bayes)\n",
        "\n",
        "ridge_bayes_ZiPS_steam_scores = scores(y_holdout, preds_ridge_bayes_ZiPS_steam)\n",
        "all_scores = scores(y_holdout, preds_all)\n",
        "\n",
        "print(\"\\nHoldout Results (RMSE, MAE, Bias):\")\n",
        "print(\"ZiPS:\", ZiPS_scores)\n",
        "print(\"ElasticNet:\", elastic_scores)\n",
        "print(\"Ridge:\", ridge_scores)\n",
        "print(\"Bayesian Ridge:\", bayes_scores)\n",
        "print(\"Steamer:\", steam_scores)\n",
        "print(\"Bayesian Ridge2:\", bayes2_scores)\n",
        "print()\n",
        "print(\"ZiPS, Ridge, Bayes, and Steamer Ensemble:\", ridge_bayes_ZiPS_steam_scores)\n",
        "print(\"All Ensemble:\", all_scores)\n",
        "print(\"ZiPS and ElasticNet:\", ZiPS_en_scores)\n",
        "print(\"ZiPS and Ridge:\", ZiPS_ridge_scores)\n",
        "print(\"ZiPS and Bayesian:\", ZiPS_bayes_scores)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NzMCc_nWTbg6",
        "outputId": "a54bc303-b693-4b9a-d8a3-97740bb3bddc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Holdout Results (RMSE, MAE, Bias):\n",
            "ZiPS: (0.0271, 0.0216, 0.0054)\n",
            "ElasticNet: (0.0276, 0.0213, -0.0067)\n",
            "Ridge: (0.0287, 0.0223, -0.0089)\n",
            "Bayesian Ridge: (0.0287, 0.0223, -0.0089)\n",
            "Steamer: (0.0284, 0.0229, 0.0083)\n",
            "Bayesian Ridge2: (0.0284, 0.0221, -0.0087)\n",
            "\n",
            "ZiPS, Ridge, Bayes, and Steamer Ensemble: (0.0266, 0.0205, -0.001)\n",
            "All Ensemble: (0.0268, 0.0206, -0.0033)\n",
            "ZiPS and ElasticNet: (0.0264, 0.0203, -0.0006)\n",
            "ZiPS and Ridge: (0.0265, 0.0205, -0.0017)\n",
            "ZiPS and Bayesian: (0.0266, 0.0205, -0.0018)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nHoldout Results (RMSE, MAE, Bias):\")\n",
        "print(\"ZiPS and ElasticNet:\", ZiPS_en_scores)\n",
        "print(\"ZiPS:\", ZiPS_scores)\n",
        "print(\"Steamer:\", steam_scores)"
      ],
      "metadata": {
        "id": "SEMzJu1nTvhy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2dc70436-5eb7-4670-f605-5c687e2cc0c6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Holdout Results (RMSE, MAE, Bias):\n",
            "ZiPS and ElasticNet: (0.0264, 0.0203, -0.0006)\n",
            "ZiPS: (0.0271, 0.0216, 0.0054)\n",
            "Steamer: (0.0284, 0.0229, 0.0083)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "preds_ZiPS_en.to_csv('Holdout_Preds_ZiPS_ElasticNet.csv')"
      ],
      "metadata": {
        "id": "q83zJZY_BURA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bootrap_scores(preds_ZiPS_en, ZiPS_preds, y_holdout, 'ZiPS')\n",
        "bootrap_scores(preds_ZiPS_en, steam_preds, y_holdout, 'Steamer')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zRCwGnWu8afE",
        "outputId": "f1fc7338-f757-469f-8777-c5ae06a37d1a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bootstrap p-value (prob ZiPS rmse <= Ensemble rmse): 0.103\n",
            "Bootstrap p-value (prob ZiPS mae <= Ensemble mae): 0.013\n",
            "Bootstrap p-value (prob Steamer rmse <= Ensemble rmse): 0.006\n",
            "Bootstrap p-value (prob Steamer mae <= Ensemble mae): 0.001\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Predict 2025**"
      ],
      "metadata": {
        "id": "l5r_fuqp9qgq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "hitters_400_PA_24 = pd.read_csv('hitters_400_PA_steamer_ZiPS_bp_25.csv')\n",
        "hitters_400_PA_24[\"Year\"] = hitters_400_PA_24[\"Year\"].astype(int)\n",
        "hitters_400_PA_24 = hitters_400_PA_24.drop_duplicates().reset_index(drop=True)\n",
        "hitters_400_PA_24 = hitters_400_PA_24.fillna(0)\n",
        "\n",
        "cols_to_drop = ['ID_ZiPS', 'ID_steamer', \"Year\",\n",
        "                'MLB_ID', 'ID']\n",
        "hitters_400_PA_24 = hitters_400_PA_24.drop(columns = cols_to_drop)\n",
        "wOBA_ZiPS_25 = hitters_400_PA_24['wOBA_ZiPS']"
      ],
      "metadata": {
        "collapsed": true,
        "id": "OxubuIhc9uR0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hitters_400_PA_24_top = hitters_400_PA_24[top_features]\n",
        "hitters_400_PA_24_top = pd.DataFrame(interaction_transformer.transform(hitters_400_PA_24_top), columns = interaction_transformer.get_feature_names_out())\n",
        "\n",
        "hitters_400_PA_24 = hitters_400_PA_24.reset_index(drop = True)\n",
        "hitters_400_PA_24 = pd.concat([hitters_400_PA_24, hitters_400_PA_24_top], axis=1)\n",
        "\n",
        "#Scale:\n",
        "hitters_400_PA_24 = pd.DataFrame(scaler2.transform(hitters_400_PA_24), columns = hitters_400_PA_24.columns)\n",
        "\n",
        "# Predictions\n",
        "preds_en_25 = best_en.predict(hitters_400_PA_24)\n",
        "preds_2025 = (preds_en_25 + wOBA_ZiPS_25)/2"
      ],
      "metadata": {
        "id": "opcqkSeYAfO3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "preds_2025.to_csv('2025 wOBA Preds.csv')"
      ],
      "metadata": {
        "id": "zjEp18KVsKKL"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}